\documentclass[10pt,mathserif]{beamer}

\input{../preambles/defs.tex}
\input{../preambles/beamer_setting}
\input{../preambles/affiliation}

\title{\large \bfseries Primal-Dual Splitting Methods}

\begin{document}

\frame{
\thispagestyle{empty}
\titlepage
}

\begin{frame}
\frametitle{Main idea}

We study techniques for deriving primal-dual methods, methods that explicitly maintain and update both primal and dual variables.

\vspace{0.2in}
Base splitting methods are limited to minimizing $f(x)+g(x)$ or $f(x)+g(x)+h(x)$.
Primal-dual methods can solve a wider range of problems and can exploit problem structures with a high level of freedom.


%and, as we later discuss in \S\ref{c:distributed}, are useful at analyzing distributed and decentralized algorithms.


% In particular, primal-dual methods can be used to decompose a function $f(x) = g(Ax)$, where $A$ is a matrix, to generate an algorithm based on $\prox_{g}$ and applying the multiplications of $A, A^\intercal$. Inversions of $A^\intercal A$ or $AA^\intercal$ are avoided, and $g$ can be non-differentiable.


\end{frame}



\section{Infimal postcomposition technique}

\begin{frame}
\frametitle{Infimal postcomposition technique}
Infimal postcomposition technique:
\vspace{0.2in}

 (i) Transform 
\[
\begin{array}{ll}
\underset{x\in \reals^{p}}{\mbox{minimize}}&f(x)+\cdots\\
\mbox{subject to}&Ax+\cdots
\end{array}
\]
into an equivalent form without constraints
\[
\begin{array}{ll}
\underset{z\in \reals^n}{\mbox{minimize}}&(A\rhd f)(z)+\cdots
\end{array}
\]
%Recast problem into  minimize $\tilde{f}(z)+\tilde{g}(z)$ using infimal postcomposition. 
%
%The infimal postcomposition technique 
using the infimal postcomposition $A\rhd f$.
\vspace{0.2in}

(ii) Apply base splittings.
\end{frame}

\begin{frame}
\frametitle{Infimal postcomposition}
Infimal postcomposition (IPC) of $f$ by $A$:
%Given a function ,
%define the function   on $\reals^m$ with
\[
(A\rhd f)(z)=\inf_{x\in \{x\,|\,Ax=z\}}f(x).
\]
To clarify, $f\colon \reals^n\rightarrow \reals\cup\{\infty\}$, $A\in \reals^{m\times n}$, and
$A\rhd f\colon \reals^m\rightarrow\reals\cup\{\pm \infty\}$.
Also called the image of $f$ under $A$.

\vspace{0.2in}
If $f$ is CCP and $\cR(A^\intercal)\cap \relint \dom f^*\ne \emptyset$, then $A\rhd f$ is CCP.
%Rockafellar 16.3
\end{frame}



\begin{frame}[label=frame_ipc_formula1]
\frametitle{IPC identity}
Identity (i):
\[
(A\rhd f)^*(u)=f^*(A^\intercal u)
\]

\vspace{0.2in}


Follows from 
\begin{align*}
(A\rhd f)^*(u)&
=\sup_{z\in \reals^m}\left\{
\langle u,z\rangle -
\inf_{x\in \reals^n}\left\{f(x)+\delta_{\{x\,|\,Ax=z\}}(x)\right\}
\right\}\nonumber\\
&=-\inf_{z\in \reals^m}\left\{
-\langle u,z\rangle +
\inf_{x\in \reals^n}\left\{f(x)+\delta_{\{x\,|\,Ax=z\}}(x)\right\}
\right\}\nonumber\\
&=-\inf_{x\in\reals^n,z\in\reals^m}\left\{
f(x)+\delta_{\{x\,|\,Ax=z\}}(x)-\langle u,z\rangle
\right\}\nonumber\\
&=-\inf_{x\in \reals^n}\left\{
f(x)-\langle u,Ax\rangle
\right\}\nonumber=f^*(A^\intercal u).
\end{align*}
%which follows from the definition of the infimal postcomposition $A\rhd f$, discussed in \S\ref{ss:infimal},
%We first define and discuss the infimal postcomposition and then demonstrate the technique.
% \[
% \begin{array}{ll}
% \underset{z\in \reals^n}{\mbox{minimize}}& \tilde{f}(z)+\tilde{g}(z)
% \end{array}
% \]
\vspace{0.2in}

Identity (i) is why we encounter the infimal postcomposition.
\end{frame}

\begin{frame}[label=frame_ipc_formula2]
\frametitle{IPC identity}

Identity (ii):
If $\mathcal{R}(A^\intercal)\cap\relint \dom f^*\ne \emptyset$, then
\[
\begin{array}{l}
\displaystyle{x\in \argmin_x \left\{f(x) + (1/2)\|Ax-y\|^2\right\}}\\
z=Ax
\end{array}
\quad\Leftrightarrow\quad
z=\prox_{A\rhd f}(y)
\]
and the $\argmin$ of the left-hand side exists.
(The $\argmin_x$ may not be unique, but $z=Ax$ is unique.)
%See Exercise~\ref{exercise:ipc_prox} for a proof.
\vspace{0.2in}

Proof in Exercise 3.1.
\end{frame}


\begin{frame}[label=frame_admm_first,plain]
\frametitle{Alternating direction method of multipliers (ADMM)}
%Let $f$ and $g$ be CCP, $A\in \reals^{n\times p}$,  $B\in \reals^{n\times q}$, and $c\in \reals^n$.
Consider the primal
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f(x)+g(y)\\
\mbox{subject to}&Ax+By=c
\end{array}
\]
and the dual problem
\[
\begin{array}{ll}
\underset{u  \in \reals^n}{\mbox{maximize}}&-f^*(-A^\intercal u )-g^*(-B^\intercal u )-c^\intercal u
\end{array}
\]
generated by the Lagrangian
\[
\lagrange(x,y,u )=f(x)+g(y)+\langle u , Ax+By-c\rangle.
\]
Assume the regularity conditions
\[
\cR(A^\intercal)\cap \relint \dom f^*\ne \emptyset\nonumber,\qquad
\cR(B^\intercal)\cap \relint \dom g^*\ne \emptyset.
\]
We use the augmented Lagrangian
\[
\lagrange_\rho (x,y,u )=f(x)+g(y)+\langle u , Ax+By-c\rangle+\frac{\rho}{2}\|Ax+By-c\|^2.
\]
\end{frame}

\begin{frame}
\frametitle{Alternating direction method of multipliers (ADMM)}
Primal problem
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f(x)+g(y)\\
\mbox{subject to}&Ax+By=c,
\end{array}
\]
 is equivalent to
\[
\begin{array}{ll}
\underset{\substack{z\in \reals^n\\x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f(x)\qquad\quad+g(y)\\
\mbox{subject to}&Ax=z,\quad z+By=c,
\end{array}
\]
which is in turn equivalent to
\[
\begin{array}{ll}
\underset{\substack{z\in\reals^n}}{\mbox{minimize}}&
\underbrace{(A\rhd f)(z)}_{=\tilde{f}(z)}
+
\underbrace{(B\rhd g)(c-z)}_{=\tilde{g}(z)}
\end{array}.
\]
\end{frame}


\begin{frame}
\frametitle{Alternating direction method of multipliers (ADMM)}
The DRS FPI with respect to
$(1/2)\opI+(1/2)\opR_{\alpha^{-1} \partial \tilde{f}}\opR_{\alpha^{-1} \partial \tilde{g}}$ is
\begin{align*}
z^{k+1/2}&=\prox_{\alpha^{-1}\tilde{g}}(\zeta^k)\\
z^{k+1}&=\prox_{\alpha^{-1}\tilde{f}}(2z^{k+1/2}-\zeta^k)\\
\zeta^{k+1}&=\zeta^k+z^{k+1}-z^{k+1/2}.
\end{align*}
% \begin{align*}
% z^{k+1/2}&=\argmin_{z}\left\{\tilde{g}(z)+\frac{\alpha}{2}\|z-\zeta^k\|^2\right\}\\
% z^{k+1}&=\argmin_{z}\left\{\tilde{f}(z)+\frac{\alpha}{2}\|z-2z^{k+1/2}+\zeta^k\|^2\right\}\\
% \zeta^{k+1}&=\zeta^k+z^{k+1}-z^{k+1/2},
% \end{align*}
%We introduce and substitute the variables $x^k$, $y^k$, and $u ^k$ defined implicitly by
Define $z^{k+1/2}=c-By^{k+1}$, $z^{k+1}=Ax^{k+2}$, and $\zeta^k=\alpha^{-1} u ^k+Ax^{k+1}$ and use identity (ii) of page \pageref{frame_ipc_formula2}:
%\begin{align*}
%y^{k+1}&=\argmin_{y}\left\{\alpha g(y)+(1/2)\|Ax^{k+1}+By-c+\alpha u ^k\|^2\right\}\\
%x^{k+2}&=\argmin_{x}\left\{\alpha f(x)+(1/2)\|Ax+By^{k+1}-c
%+(\alphau ^k+Ax^{k+1}+By^{k+1}-c)
%\|^2\right\}\\
% u ^{k+1}
%&= u ^k+(1/\alpha)(Ax^{k+1}+By^{k+1}-c)
%\end{align*}
\begin{align*}
y^{k+1}&\in\argmin_{y}\left\{ g(y)
+\langle u ^k,Ax^{k+1}+By-c\rangle+
\frac{\alpha}{2}\|Ax^{k+1}+By-c\|^2\right\}\\
x^{k+2}&\in\argmin_{x}\left\{ f(x)+
 \langle u ^{k+1},Ax+By^{k+1}-c\rangle+
\frac{\alpha}{2}\|Ax+By^{k+1}-c
\|^2\right\}\\
 u ^{k+1}&= u ^k+\alpha (Ax^{k+1}+By^{k+1}-c)
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Alternating direction method of multipliers (ADMM)}
Reorder updates:
\begin{align*}
x^{k+1}&\in\argmin_{x}\left\{
f(x)+\langle u ^k,Ax+By^k-c\rangle+
\frac{\alpha}{2}\|Ax+By^k-c\|^2
\right\}\\
y^{k+1}&\in\argmin_{y}\left\{ g(y)
+\langle u ^k,Ax^{k+1}+By-c\rangle+
\frac{\alpha}{2}\|Ax^{k+1}+By-c\|^2\right\}\\
 u ^{k+1}&= u ^k+\alpha (Ax^{k+1}+By^{k+1}-c)
\end{align*}
Write updates more concisely:
\begin{align*}
x^{k+1}&\in\argmin_{x}\lagrange_{\alpha}(x,y^k,u ^k) \\%\label{eq:ADMMx}\\
y^{k+1}&\in\argmin_{y}\lagrange_{\alpha}(x^{k+1},y,u ^k)\\% \label{eq:ADMMy}\\
 u ^{k+1}&= u ^k+\alpha (Ax^{k+1}+By^{k+1}-c)%. \label{eq:ADMMnu}
%x^{k+2}&=\argmin_{x}\left\{ f(x)+ (u ^{k+1})^\intercal(Ax+By^{k+1}-c)+\frac{\alpha}{2}\|Ax+By^{k+1}-c\|^2\right\}.
\end{align*}
This is the alternating direction methods of multipliers (ADMM).
\end{frame}

\begin{frame}
\frametitle{Convergence analysis: ADMM}


We have completed the core of the convergence analysis, but bookkeeping remains: check conditions and translate the convergence of DRS into the convergence of ADMM.

\vspace{0.2in}



DRS requires total duality between
\[
\begin{array}{ll}
\underset{\substack{z\in\reals^n}}{\mbox{minimize}}&
(A\rhd f)(z)+(B\rhd g)(c-z)
\end{array}
\]
and
\[
\begin{array}{ll}
\underset{u  \in \reals^n}{\mbox{maximize}}&-f^*(-A^\intercal u )-g^*(-B^\intercal u )-c^\intercal u
\end{array}
\]
generated by the Lagrangian
\[
\tilde{\lagrange}(z,u )=(A\rhd f)(z)+\langle z,u\rangle -g^*(-B^\intercal u )-c^\intercal u.
\]


\vspace{0.2in}
We need total duality with $\tilde{\lagrange}$, rather than $\lagrange$.
%DRS for optimization converges under total duality defined for a specific Lagrangian that is not the 
\end{frame}


\begin{frame}[fragile]
\frametitle{Convergence analysis: ADMM}
If
\begingroup\makeatletter\def\f@size{6}\check@mathfonts
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f(x)+g(y)\\
\mbox{subject to}&Ax+By=c,
\end{array}
\begin{array}{ll}
\underset{u  \in \reals^n}{\mbox{maximize}}&-f^*(-A^\intercal u )-g^*(-B^\intercal u )-c^\intercal u 
\end{array}
\]
\endgroup
have solutions $(x^\star,y^\star)$ and $u ^\star$ for which strong duality holds
then
\begingroup\makeatletter\def\f@size{6}\check@mathfonts
\[
\begin{array}{ll}
\underset{\substack{z\in\reals^n}}{\mbox{minimize}}&
(A\rhd f)(z)+(B\rhd g)(c-z),
\end{array}
\begin{array}{ll}
\underset{u  \in \reals^n}{\mbox{maximize}}&-f^*(-A^\intercal u )-g^*(-B^\intercal u )-c^\intercal u
\end{array}
\]
\endgroup
have solutions $z^\star=Ax^\star$  and $u ^\star$ for which strong duality holds.\\
I.e., [total duality original problem] $\Rightarrow$ [total duality equivalent problem]




\vspace{0.2in}

If total duality between the original primal and dual problems holds,
the regularity condition of page \pageref{frame_admm_first} holds, and $\alpha>0$,
then ADMM is well-defined, $Ax^k\rightarrow Ax^\star$, and $By^k\rightarrow By^\star$.
\end{frame}



\begin{frame}
\frametitle{Discussion: Regularity condition}

Regularity condition of page \pageref{frame_admm_first} ensures
(i) $A\rhd f$ and $B\rhd g$ are CCP and
(ii) minimizers defining the iterations exist.
%(DRS applied to non-CCP (but convex) functions can run into pathologies.)

\end{frame}


\section{Dualization technique}
\begin{frame}
\frametitle{Dualization technique}

Dualization technique: apply base splittings to the dual.
\vspace{0.2in}

Certain primal problems with constraints have duals without constraints.
We have seen this technique with the method of multipliers.
\end{frame}


\begin{frame}[fragile]
\frametitle{Alternating direction method of multipliers (ADMM)}
Alternate derivation of ADMM.
Again consider
\begingroup\makeatletter\def\f@size{6}\check@mathfonts
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f(x)+g(y)\\
\mbox{subject to}&Ax+By=c,
\end{array}
\begin{array}{ll}
\underset{u  \in \reals^n}{\mbox{maximize}}&-\underbrace{f^*(-A^\intercal u )}_{=\tilde{f}( u )}-\underbrace{(g^*(-B^\intercal u )+c^\intercal u )}_{=\tilde{g}( u )}
\end{array}
\]
\endgroup
generated by
\[
\lagrange(x,y,u )=f(x)+g(y)+\langle u , Ax+By-c\rangle.
\]
%We use the augmented Lagrangian \eqref{eq:admm-augmented}.

\vspace{0.2in}
Apply DRS to dual:
FPI with  $\frac{1}{2}\opI+\frac{1}{2}\opR_{\alpha \partial \tilde{f}}\opR_{\alpha \partial \tilde{g}}$, is
\begin{align*}
\mu^{k+1/2}&=\opJ_{\alpha \partial \tilde{g}}(\psi^k)\\
\mu^{k+1}&=\opJ_{\alpha \partial \tilde{f}}(2\mu^{k+1/2}-\psi^k)\\
\psi^{k+1}&=\psi^k+\mu^{k+1}-\mu^{k+1/2}.
\end{align*}
\end{frame}


\begin{frame}[label=frame_admm2]
\frametitle{Alternating direction method of multipliers (ADMM)}
Using $\opJ_{\alpha(\opA(\cdot)+t)}(u)=\opJ_{\alpha \opA}(u-\alpha t)$ and 
\begin{align*}
\label{eq:J_conj}
v=\prox_{\alpha f^*(A^\intercal \cdot)}(u)
\quad\Leftrightarrow\quad
\begin{array}{l}
x\in\argmin_x\left\{
f(x)-\langle u,Ax\rangle +\frac{\alpha}{2}\|Ax\|^2
\right\}\\
v= u-\alpha Ax,
\end{array}
\end{align*}
 write out resolvent evaluations:
\begin{align*}
\tilde{y}^{k+1}&\in\argmin_y\left\{g(y)+\langle \psi^k-\alpha c,By\rangle +\frac{\alpha}{2}\|By\|_2^2\right\}\\
\mu^{k+1/2}&=\psi^k+\alpha (B\tilde{y}^{k+1}-c)\\
\tilde{x}^{k+1}&\in\argmin_x\left\{f(x)+\langle \psi^k+2\alpha( B\tilde{y}^{k+1}-c),Ax\rangle+\frac{\alpha}{2}\|Ax\|_2^2\right\}\\
\mu^{k+1}&=\psi^k+\alpha A\tilde{x}^{k+1}+2\alpha (B\tilde{y}^{k+1}-c)\\
\psi^{k+1}&=\psi^k+\alpha(A\tilde{x}^{k+1}+B\tilde{y}^{k+1}-c)
\end{align*}
\end{frame}


\begin{frame}
\frametitle{Alternating direction method of multipliers (ADMM)}
Eliminate $\mu^{k+1/2}$ and $\mu^{k+1}$ and reorganize:
\begin{align*}
\tilde{y}^{k+1}&\in\argmin_y\left\{g(y)+\langle \psi^k-\alpha A\tilde{x}^k,By\rangle +\frac{\alpha}{2}\|A\tilde{x}^k+By-c\|_2^2\right\}\nonumber\\
\tilde{x}^{k+1}&\in\argmin_x\left\{f(x)+\langle \psi^k+\alpha( B\tilde{y}^{k+1}-c),Ax\rangle+\frac{\alpha}{2}\|Ax+B\tilde{y}^{k+1}-c\|_2^2\right\}\nonumber\\
\psi^{k+1}&=\psi^k+\alpha(A\tilde{x}^{k+1}+B\tilde{y}^{k+1}-c)
\end{align*}
%Although this is not yet ADMM, this is a legitimate method.
% Since $\mu^{k+1/2}\rightarrow u ^\star$ and
%  $\mu^{k+1}\rightarrow u ^\star$ for some $ u ^\star$,
%  this implies
%  $A\tilde{x}^{k+1}+B\tilde{y}^{k+1}-c\rightarrow 0$
Substitute $ u ^k=\psi^k-\alpha A\tilde{x}^k$:
\begin{align*}
\tilde{y}^{k+1}&\in\argmin_y\left\{g(y)
+\langle u ^k,By\rangle
+\frac{\alpha}{2}\|A\tilde{x}^k+By-c\|_2^2\right\}\\
\tilde{x}^{k+1}&\in\argmin_x\left\{f(x)
+\langle u ^{k+1},Ax\rangle
+\frac{\alpha}{2}\|Ax+B\tilde{y}^{k+1}-c\|_2^2\right\}\\
 u ^{k+1}&= u ^k+\alpha(A\tilde{x}^{k}+B\tilde{y}^{k+1}-c)
\end{align*}
\end{frame}


\begin{frame}
\frametitle{Alternating direction method of multipliers (ADMM)}
Reorder the updates and substitute $x^{k+1}=\tilde{x}^k$ and $y^k=\tilde{y}^k$:
\begin{align*}
% x^{k+1}&=
% \argmin_{x}\left\{
% f(x)+\frac{\alpha}{2}\|Ax+Bz^k-c+u^k\|_2^2\right\}\\
% z^{k+1}&=
% \argmin_{z}\left\{
% g(z)+\frac{\alpha}{2}\|Ax^{k+1}+Bz-c+u^k\|_2^2\right\}\\
x^{k+1}&\in
 \argmin_{x} \lagrange_\alpha(x,y^k, u ^k)\\
 y^{k+1}&\in
 \argmin_{y} \lagrange_\alpha(x^{k+1},y, u ^k)\\
 u ^{k+1}&= u ^k+\alpha(Ax^{k+1}+By^{k+1}-c)
\end{align*}
%This method is called the \emph{alternating direction method of multipliers} (ADMM).

\vspace{0.2in}
If total duality, the regularity condition of page \pageref{frame_admm_first}, and $\alpha>0$ hold, then $ u ^k\rightarrow  u ^\star$, $Ax^k\rightarrow Ax^\star$, and $By^k\rightarrow By^\star$.

\vspace{0.2in}
Convergence analysis: The previous analysis with IPC established $Ax^k\rightarrow Ax^\star$ and $By^k\rightarrow By^\star$.
Since $\mu^{k+1/2}\rightarrow  u ^\star$, this implies $\psi^k\rightarrow  u ^\star+\alpha Ax^\star$ and $ u ^k\rightarrow  u ^\star$.
\end{frame}



\begin{frame}
\frametitle{Remark: Multiple derivations}
For some methods, we present multiple derivations.
E.g.\ we derive PDHG with variable metric PPM, with BCV, and from linearized ADMM.

\vspace{0.2in}
Different derivations provide related but distinct interpretations.\\
They show intimate connection between various primal-dual methods.
\end{frame}


\begin{frame}[fragile]
\frametitle{Alternating minimization algorithm (AMA)}
Again consider
\begingroup\makeatletter\def\f@size{6}\check@mathfonts
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f(x)+g(y)\\
\mbox{subject to}&Ax+By=c,
\end{array}
\begin{array}{ll}
\underset{u  \in \reals^n}{\mbox{maximize}}&-\underbrace{f^*(-A^\intercal u )}_{=\tilde{f}( u )}-\underbrace{(g^*(-B^\intercal u )+c^\intercal u )}_{=\tilde{g}( u )}
\end{array}
\]
\endgroup
generated by the Lagrangian 
\[
\lagrange(x,y,u )=f(x)+g(y)+\langle u , Ax+By-c\rangle.
\]
Assume regularity conditions of page \pageref{frame_admm_first}.

\vspace{0.2in}
Further assume $f$ is $\mu$-strongly convex, which implies \\
$f^*(-A^\intercal u )$ is $\frac{\lambda_{\mathrm{max}}(A^\intercal A)}{\mu}$-smooth.
\end{frame}

\begin{frame}
\frametitle{Alternating minimization algorithm (AMA)}
Apply FBS to the dual.
FPI with $(\opI+\alpha \partial \tilde{g})^{-1}(\opI-\alpha \nabla \tilde{f})$ is
\begin{align*}
 u ^{k+1/2}&= u ^k-\alpha \nabla \tilde{f}( u ^k)\\
 u ^{k+1}&=
(I+\alpha\partial \tilde{g})^{-1}( u ^{k+1/2}).
\end{align*}

Using the identities re-stated in page~\pageref{frame_admm2} and
\[
u\in \partial (f^*(A^\intercal \cdot))(y)
\quad\Leftrightarrow\quad
\begin{array}{l}
x\in \argmin_z\left\{
f(z)-\langle y, Az\rangle
\right\}\\
u= A x
\end{array}
\]
write out gradient and resolvent evaluations:
\begin{align*}
x^{k+1}&=\argmin_x\left\{f(x)+\langle  u ^k,Ax\rangle \right\}\\
 u ^{k+1/2}&= u ^k+\alpha Ax^{k+1}\\
y^{k+1}&\in\argmin_y\left\{ g(y)+\langle  u ^{k+1/2}-\alpha c,By\rangle +\frac{\alpha}{2}\|By\|^2\right\}\\
 u ^{k+1}&= u ^{k+1/2}+\alpha By^{k+1}-\alpha c
\end{align*}
\end{frame}


\begin{frame}
\frametitle{Alternating minimization algorithm (AMA)}
Simplify iteration:
\begin{align*}
x^{k+1}&=\argmin_x \lagrange(x,y^k, u ^k)\\
y^{k+1}&\in\argmin_y \lagrange_\alpha (x^{k+1},y, u ^k)\\
 u ^{k+1}&= u ^k+\alpha(Ax^{k+1}+By^{k+1}-c).
\end{align*}
This is alternating minimization algorithm (AMA) or dual proximal gradient.

\vspace{0.2in}
If total duality, regularity conditions of page \pageref{frame_admm_first}, $\mu$-strongly convex of $f$, and $\alpha\in (0,2\mu/\lambda_{\mathrm{max}}(A^\intercal A))$ hold,
then $ u ^k\rightarrow  u ^\star$, $x^{k}\rightarrow x^\star$,
and $By^k\rightarrow By^\star$.
\end{frame}


\begin{frame}
\frametitle{Convergence analysis: AMA}
\begin{enumerate}
\item 
Since FBS converges, $ u ^k\rightarrow  u ^\star$.
\item 

[$(x^\star,y^\star, u ^\star)$ is a saddle point] $\Rightarrow$ 
[$x^\star=\argmin_x\lagrange(x,y^\star, u ^\star)$] \\$\Rightarrow$ 
[$0\in \partial f(x^\star)+A^\intercal u ^\star$] $\Rightarrow$ 
[$x^\star=\nabla f^*(-A^\intercal u ^\star)$].
\item
Since $x^{k+1}=\nabla f^*(-A^\intercal u ^k)$ and $\nabla f^*$ continuous, $ u ^k\rightarrow  u ^\star$ implies $x^k\rightarrow x^\star$.
\item

[$ u ^k\rightarrow  u ^\star$]  $\Rightarrow$ 
[$ u ^{k+1}- u ^k\rightarrow 0$] $\Rightarrow$ 
[$Ax^{k+1}+By^{k+1}-c\rightarrow 0$]\\ $\Rightarrow$ 
[$By^{k}\rightarrow By^{\star}$].
\end{enumerate}
\end{frame}



\section{Variable metric technique}
\begin{frame}
\frametitle{Variable metric technique}


Variable metric technique: use variable metric PPM or FBS with $M$ carefully chosen to cancels out certain terms.
\end{frame}

\begin{frame}[label=frame_pdhg1]
\frametitle{PDHG}
%Let $f$ and $g$ be CCP functions and $A\in\reals^{m\times n}$. 
Consider 
\[
\begin{array}{ll}
\underset{x\in \reals^n}{\mbox{minimize}}&f(x)+g(Ax),
\end{array}
\qquad
\begin{array}{ll}
\underset{ u  \in \reals^m}{\mbox{maximize}}&-f^*(-A^\intercal u )-g^*( u )
\end{array}
\]
generated by the Lagrangian %\eqref{eq:pdhg-lagrangian},
\[
\lagrange(x, u )=f(x)+\langle  u , Ax\rangle-g^*( u ).
\]
\end{frame}

\begin{frame}[label=frame_pdhg2]
\frametitle{PDHG}
Apply variable metric PPM to
\[
\partial \lagrange(x, u )
=
\begin{bmatrix}
0&A^\intercal\\
-A&0
\end{bmatrix}
\begin{bmatrix}
x\\
 u 
\end{bmatrix}
+
\begin{bmatrix}
\partial f(x)\\
\partial g^*( u )
\end{bmatrix}
\]
with
\[
M=
\begin{bmatrix}
(1/\alpha) I &-A^\intercal\\
-A &(1/\beta) I
\end{bmatrix}.
\]
 $M\succ 0$ if $\alpha,\beta>0$ and $\alpha\beta\lambda_\mathrm{max}(A^\intercal A)<1$.
\vspace{0.2in}


FPI with $(M+\partial \lagrange)^{-1}M$ is
\begin{align*}
\begin{bmatrix}
x^{k+1}\\
 u ^{k+1}
\end{bmatrix}=
\left(
\begin{bmatrix}
(1/\alpha) I&0\\
-2A&(1/\beta) I
\end{bmatrix}
+
\begin{bmatrix}
\partial f\\
\partial g^*
\end{bmatrix}
\right)^{-1}
\begin{bmatrix}
(1/\alpha) x^k-A^\intercal u ^k\\
-Ax^k+(1/\beta)  u ^k
\end{bmatrix},
\end{align*}
which is equivalent to
\begin{align*}
\begin{bmatrix}
(1/\alpha) I&0\\
-2A&(1/\beta) I
\end{bmatrix}
\begin{bmatrix}
x^{k+1}\\
 u ^{k+1}
\end{bmatrix}
+
\begin{bmatrix}
\partial f(x^{k+1})\\
\partial g^*( u ^{k+1})
\end{bmatrix}
\ni
\begin{bmatrix}
(1/\alpha) x^k-A^\intercal u ^k\\
-Ax^k+(1/\beta)  u ^k
\end{bmatrix}.
\end{align*}
\end{frame}

\begin{frame}
\frametitle{PDHG}
Linear system is lower triangular, so compute $x^{k+1}$ first and then $ u ^{k+1}$:
\begin{align*}
x^{k+1}&=
\prox_{\alpha f}(x^k-\alpha A^\intercal u ^k)\\
 u ^{k+1}&=
\prox_{\beta g^*}( u ^k+\beta A(2x^{k+1}-x^k))
\end{align*}
This is primal-dual hybrid gradient (PDHG) or Chambolle--Pock.

\vspace{0.2in}
If total duality holds,
$\alpha>0$, $\beta>0$, and
$\alpha\beta\lambda_\mathrm{max}(A^\intercal A)<1$,
then $x^k\rightarrow x^\star$ and $ u ^k\rightarrow  u ^\star$.
%where $x^\star$ and $ u ^\star$ are primal and dual solutions, respectively.
\end{frame}

\begin{frame}
\frametitle{Choice of metric}
Although PDHG is derived from PPM, which is technically not an operator splitting, PDHG is a splitting since $f$ and $g$ are split.


\vspace{0.2in}
Choosing $M$ to obtain a lower triangular system is crucial.
For example, FPI $(x^{k+1}, u ^{k+1})=(\opI+\partial \lagrange)^{-1}(x^{k}, u ^{k})$ is not useful;
off-diagonal terms couple $x^{k+1}$ and $ u ^{k+1}$ requiring simultaneous computation.
With no splitting,  one iteration is no easier than the whole problem.
\end{frame}

\begin{frame}
\frametitle{Condat--V\~u}
Consider
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^n}}{\mbox{minimize}}&f(x)+h(x)+g(Ax)
\end{array}
\begin{array}{ll}
\underset{ u  \in \reals^m}{\mbox{maximize}}&-(f+h)^*(-A^\intercal u )-g^*( u ),
\end{array}
\]
where $h$ is differentiable,
generated by
\[
\lagrange(x, u )=f(x)+h(x)+\langle  u , Ax\rangle-g^*( u ).
\]
%If the constraint qualification $(A\dom f)\cap \relint \dom g  \ne \emptyset$ holds, then $d^\star=p^\star$.

\vspace{0.2in}
Generalizes PDHG setup.
\end{frame}

\begin{frame}
\frametitle{Condat--V\~u}
Apply variable metric FBS to $\partial \lagrange$ with $M$ of page~\pageref{frame_pdhg2} with splitting
\[
\partial \lagrange(x, u )
=
\underbrace{
\begin{bmatrix}
\nabla h(x)\\
0
\end{bmatrix}}
_{=\opH(x, u )}
+
\underbrace{
\begin{bmatrix}
0&A^\intercal\\
-A&0
\end{bmatrix}
\begin{bmatrix}
x\\
 u 
\end{bmatrix}
+
\begin{bmatrix}
\partial f(x)\\
\partial g^*( u )
\end{bmatrix}
}
_{=\opF(x, u )}.
\]
 FPI with $(x^{k+1}, u ^{k+1})=(M+\opF)^{-1}(M-\opH)(x^{k}, u ^{k})$ is
\[
\begin{bmatrix}
x^{k+1}\\
 u ^{k+1}
\end{bmatrix}=
\left(
\begin{bmatrix}
(1/\alpha) I&0\\
-2A&(1/\beta) I
\end{bmatrix}
+
\begin{bmatrix}
\partial f\\
\partial g^*
\end{bmatrix}
\right)^{-1}
\begin{bmatrix}
(1/\alpha) x^k-A^\intercal u ^k-\nabla h(x^k)\\
-Ax^k+(1/\beta)  u ^k
\end{bmatrix}.
\]
\end{frame}

\begin{frame}
\frametitle{Condat--V\~u}
Again, compute $x^{k+1}$ first and then $ u ^{k+1}$:
\begin{align*}
x^{k+1}&=
\prox_{\alpha f}(x^k-\alpha A^\intercal u ^k-\alpha \nabla h(x^k))\\
 u ^{k+1}&=
\prox_{\beta g^*}( u ^k+\beta A(2x^{k+1}-x^k))
\end{align*}
This is Condat--V\~u.
If total duality holds, $h$ is $L$-smooth, $\alpha>0$, $\beta>0$, and
$\alpha L/2+\alpha \beta \lambda_\mathrm{max}(A^\intercal A)<1$,
then $x^k\rightarrow x^\star$ and $ u ^k\rightarrow  u ^\star$.
\end{frame}





\begin{frame}
\frametitle{Convergence analysis: Condat--V\~u}
Note $M\succ 0$ under the stated conditions.
%under the assumption $\alpha,\beta>0$ and \eqref{Condat-Vu-cond}.
%$\alpha L/2+\alpha \beta \lambda_\mathrm{max}(A^\intercal A)<1$.
With basic computation,
\[
M^{-1}=
\begin{bmatrix}
\alpha (I-\alpha\beta A^\intercal A)^{-1}& \alpha \beta A^\intercal (I-\alpha\beta AA^\intercal)^{-1}\\
\alpha\beta A (I-\alpha\beta A^\intercal A)^{-1}&
\beta (I-\alpha\beta AA^\intercal)^{-1}
\end{bmatrix}.
\]

Let
\[
\theta=
\frac{2}{L}\left(\frac{1}{\alpha}-\beta \lambda_\mathrm{max}(A^\intercal A)\right)
>1.
\]
($\theta>1$ equivalent to $\alpha L/2+\alpha \beta \lambda_\mathrm{max}(A^\intercal A)<1$.)

\vspace{0.2in}
\begin{align*}
\theta\left(\frac{1}{\alpha} I-\beta A^\intercal A\right)^{-1}
\preceq 
\theta\left(\frac{1}{\alpha} -\beta \lambda_\mathrm{max}(A^\intercal A)\right)^{-1}
\!\!\!I
=
\frac{2}{L}I
%\frac{2}{L}\left(\frac{1}{\alpha}-\beta \lambda_\mathrm{max}(A^\intercal A)\right) 
%\left(\frac{1}{\alpha} I-\beta A^\intercal A\right)^{-1}\\
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Convergence analysis: Condat--V\~u}
If $\opI-\theta M^{-1}\opH$ is nonexpansive in $\|\cdot\|_M$, then $\opI- M^{-1}H$ is averaged in $\|\cdot\|_M$ and Condat--V\~u converges.
\vspace{0.2in}

Nonexpansiveness of $\opI-\theta M^{-1}\opH$ in $\|\cdot\|_M$:
\begin{align*}
&\|(\opI-\theta M^{-1}\opH)(x, u )-
(\opI-\theta M^{-1}\opH)(y,v )\|^2_M\\
&=
\|(x, u )-(y,v )\|^2_M\\
&\quad-2\theta
\langle (x, u )-(y,v ),
\opH(x, u )-\opH(y,v )\rangle
+
\theta^2\|\opH(x, u )-\opH(y,v )\|^2_{M^{-1}}\\
&=
\|(x, u )-(y,v )\|^2_M\\
&\quad-2\theta
\langle x-y,\nabla h(x)-\nabla h(y)\rangle
+
\theta^2\|\nabla h(x)-\nabla h(y)\|^2_{\alpha (I-\alpha\beta A^\intercal A)^{-1}}\\
&\le
\|(x, u )-(y,v )\|^2_M\\
&\quad-(2\theta/L)
\|\nabla h(x)-\nabla h(y)\|^2
+
\theta^2\|\nabla h(x)-\nabla h(y)\|^2_{(\alpha^{-1} I-\beta A^\intercal A)^{-1}}\\
&\le
\|(x, u )-(y,v )\|^2_M.%\\
% &\quad-\theta\left(2/L-\theta(\alpha^{-1}-\beta \lambda_\mathrm{max}(A^\intercal A))^{-1}-2/L\right)
% \|\nabla h(x)-\nabla h(y)\|^2\\
% &=
% \|(x, u )-(y,v )\|^2_M.
\end{align*}
% Since $I-\theta M^{-1}H$ is nonexpansive in $\|\cdot\|_M$ for $\theta>1$,
% $I- M^{-1}H$ is averaged  in $\|\cdot\|_M$.
%This establishes that variable metric FBS converges.
\end{frame}

\begin{frame}
\frametitle{Example: Computational tomography (CT)}
In computational tomography (CT), the medical device measures the (discrete) Radon transform of a patient.
The Radon transform is a linear operator $R\in \reals^{m\times n}$ and $b\in \reals^m$ is the measurement.


\vspace{0.2in}

Usually $m<n$ (more unknowns than measurements) and $b\approx Rx^\mathrm{true}$ due to measurement noise. 
Image is recovered with
\[
\begin{array}{ll}
\underset{x\in \reals^n}{\mbox{minimize}}&
\frac{1}{2}\|Rx-b\|^2+\lambda \|Dx\|_1
\end{array}
\]
where the optimization variable $x\in \reals^n$ represents the 2D image to recover,
$D$ is the 2D finite difference operator, and $\lambda>0$.

\vspace{0.2in}

$R^\intercal$ is called backprojection.
$R$ and $D$ are large matrices, but application of them and their transposes are efficient.
\end{frame}

\begin{frame}
\frametitle{Example: Computational tomography (CT)}
Problem is equivalent to
\[
\begin{array}{ll}
\underset{x\in \reals^n}{\mbox{minimize}}&
0(x)+g(Ax),
\end{array}
\]
where
\[
A=
\begin{bmatrix}
R\\(\beta/\alpha)D
\end{bmatrix},\qquad
g(y,z)=\frac{1}{2}\|y-b\|^2+(\lambda\alpha/\beta) \|z\|_1
\]
for any $\alpha,\beta>0$.
PDHG applied to this problem is
\begin{align*}
x^{k+1}&=x^k
-(1/\alpha)(\alpha R^\intercal u^k+\beta D^\intercal v^k)
\\
%(A^\intercal u^k+D^\intercal v^k)\\
u^{k+1}&=
\frac{1}{1+\alpha}(u^k+\alpha R(2x^{k+1}-x^k)-\alpha b)\\
v^{k+1}&=\Pi_{[-\lambda\alpha/\beta,\lambda\alpha/\beta]}\left(v^k+\beta D(2x^{k+1}-x^k)\right).
\end{align*}

\end{frame}



\section{Gaussian elimination technique}
\begin{frame}
\frametitle{Gaussian elimination technique}
Gaussian elimination technique: make inclusions upper or lower triangular by multiplying by an invertible matrix.
%\vspace{0.2in}


%The lower or upper triangular system are then solved sequentially, in a split manner.
%We had seen this technique when in computing the resolvent of the saddle subdifferential.
\end{frame}

\begin{frame}
\frametitle{Proximal method of multipliers\\ with function linearization}
Consider primal problem
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^n}}{\mbox{minimize}}&f(x)+h(x)\\
\mbox{subject to}&Ax=b,
\end{array}
\]
where $h$ is differentiable, generated by the Lagrangian
\begin{equation*}
\lagrange(x, u )=f(x)+h(x)+\langle  u , Ax-b\rangle.
%\label{eq:PAPC-lagrangian}
\end{equation*}
\vspace{0.2in}

Split saddle subdifferential:
\[
\partial \lagrange(x, u )
=
\underbrace{
\begin{bmatrix}
\nabla h(x)\\
b
\end{bmatrix}}
_{=\opH(x, u )}
+
\underbrace{
\begin{bmatrix}
0&A^\intercal\\
-A&0
\end{bmatrix}
\begin{bmatrix}
x\\
 u 
\end{bmatrix}
+
\begin{bmatrix}
\partial f(x)\\
0
\end{bmatrix}
}
_{=\opG(x, u )}.
\]
% Use the metric matrix
% \begin{align*}
%   \overline{M} =\begin{bmatrix}
%           (1/\alpha)I & 0 \\
%           0 & (1/\beta)I
%         \end{bmatrix}.
% \end{align*}
\end{frame}





\begin{frame}
\frametitle{Proximal method of multipliers\\ with function linearization}
FPI with $(\opI+\alpha \opG)^{-1}(\opI-\alpha \opH)$:
\begin{align*}
  \begin{bmatrix}
    I & \alpha A^\intercal  \\
    -\alpha A & I
  \end{bmatrix}
  \begin{bmatrix}
        x^{k+1} \\
         u ^{k+1}
  \end{bmatrix}
  +
  \begin{bmatrix}
  \alpha \partial f(x^{k+1})\\
  0
  \end{bmatrix}
  \ni
  \begin{bmatrix}
  x^k-\alpha \nabla h(x^k)\\
   u ^k-\alpha b
  \end{bmatrix}
\end{align*}
\vspace{0.2in}

Left-multiply with invertible matrix
\[
\begin{bmatrix}
I&-\alpha  A^\intercal\\
0&I
\end{bmatrix},
\]
which corresponds to Gaussian elimination:
\begin{align*}
  &\begin{bmatrix}
    I+\alpha^2 A^\intercal A & 0  \\
    -\alpha A & I
  \end{bmatrix}
  \begin{bmatrix}
        x^{k+1} \\
         u ^{k+1}
  \end{bmatrix}
  +
  \begin{bmatrix}
  \alpha \partial f(x^{k+1})\\
  0
  \end{bmatrix}\\
  &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ni
  \begin{bmatrix}
  x^k-\alpha  \nabla  h(x^k)-\alpha A^\intercal( u ^k-\alpha  b)\\
   u ^k-\alpha b
  \end{bmatrix}
  \end{align*}
\end{frame}

\begin{frame}[fragile]
\frametitle{Proximal method of multipliers\\ with function linearization}
Compute $x^{k+1}$ first and then compute $ u ^{k+1}$:
\begin{align*}
  \label{eq:pmmfl_x}
    x^{k+1} & = \argmin_x \bigg\{f(x) + \langle\nabla h(x^k),x\rangle +\langle  u ^k, Ax-b\rangle + \frac{\alpha }{2}\|Ax-b\|^2 \\
    &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+ \frac{1}{2\alpha}\|x-x^k\|^2\bigg\}\\[-0.5em]
     u ^{k+1} & =  u ^k + \alpha (Ax^{k+1}-b)
\end{align*}
This is proximal method of multipliers with function linearization.
\vspace{0.2in}


If total duality holds, $h$ is $L$-smooth, and $\alpha\in(0,2/L)$, then $x^k\to x^\star$ and $u^k\to u^\star$.
\end{frame}

\begin{frame}
\frametitle{PAPC/PDFP$^2$O}
Consider primal problem
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^n}}{\mbox{minimize}}&h(x)+g(Ax)
\end{array}
%\qquad
%\begin{array}{ll}
%\underset{ u  \in \reals^m}{\mbox{maximize}}&-h^*(-A^\intercal u )-g^*( u ),
%\end{array}
\]
where $h$ is differentiable, and the Lagrangian
\begin{equation*}
\lagrange(x, u )=h(x)+\langle  u , Ax\rangle-g^*( u ).
%\label{eq:PAPC-lagrangian}
\end{equation*}

\vspace{0.2in}
Apply variable metric FBS to $\partial \lagrange$ and use Gaussian elimination technique.
Split
\[
\partial \lagrange(x, u )
=
\underbrace{
\begin{bmatrix}
\nabla h(x)\\
0
\end{bmatrix}}
_{=\opH(x, u )}
+
\underbrace{
\begin{bmatrix}
0&A^\intercal\\
-A&0
\end{bmatrix}
\begin{bmatrix}
x\\
 u 
\end{bmatrix}
+
\begin{bmatrix}
0\\
\partial g^*( u )
\end{bmatrix}
}
_{=\opG(x, u )}
\]
and use
\begin{align*}
  M =\begin{bmatrix}
          (1/\alpha)I & 0 \\
          0 & (1/\beta)I - \alpha A A^\intercal
        \end{bmatrix},
\end{align*}
which satisfies $M\succ 0$ if $\alpha\beta\lambda_\mathrm{max}(A^\intercal A)<1$.
\end{frame}

\begin{frame}
\frametitle{PAPC/PDFP$^2$O}
FPI with $(M+\opG)^{-1}(M-\opH)$ is described by
% \[
% (x^{k+1}, u ^{k+1})=
% (M+G)^{-1}(M-H)(x^{k}, u ^{k})
% \]
% becomes
\begin{align*}
  \begin{bmatrix}
    (1/\alpha)I & A^\intercal  \\
    -A & (1/\beta)I -\alpha AA^\intercal
  \end{bmatrix}
  \begin{bmatrix}
        x^{k+1} \\
         u ^{k+1}
  \end{bmatrix}
  +
  \begin{bmatrix}
  0\\
  \partial g^*( u ^{k+1})
  \end{bmatrix}
  \ni
  \begin{bmatrix}
  (1/\alpha)x^k-\nabla h(x^k)\\
  (1/\beta) u ^k-\alpha A A^\intercal u ^k
  \end{bmatrix}
  .
\end{align*}
% \begin{align*}
%   \begin{bmatrix}
%     (1/\alpha)I & A^\intercal  \\
%     -A & (1/\beta)I -\alpha AA^\intercal
%   \end{bmatrix}
%   \begin{bmatrix}
%         x^{k+1} \\
%          u ^{k+1}
%   \end{bmatrix}
%   +
%   \begin{bmatrix}
%   0\\
%   \partial g^*( u ^{k+1})
%   \end{bmatrix}
%   \ni
%   \begin{bmatrix}
%     (1/\alpha)I &  0\\
%      0& (1/\beta)I - \alpha AA^\intercal
%   \end{bmatrix}
%   \begin{bmatrix}
%         x^{k} \\
%          u ^{k}
%   \end{bmatrix}
%   -\begin{bmatrix}
%      \nabla h(x^k) \\
%      0
%   \end{bmatrix}.
% \end{align*}
Left-multiply the system with the invertible matrix
\[
\begin{bmatrix}
I&0\\
\alpha A&I
\end{bmatrix},
\]
which corresponds to Gaussian elimination,
and get
\begin{align*}
  &\begin{bmatrix}
    (1/\alpha)I & A^\intercal  \\
    0 & (1/\beta)I
  \end{bmatrix}
  \begin{bmatrix}
        x^{k+1} \\
         u ^{k+1}
  \end{bmatrix}
  +
  \begin{bmatrix}
  0\\
  \partial g^*( u ^{k+1})
  \end{bmatrix}\\
  &\qquad\qquad\qquad\qquad\qquad\ni
    \begin{bmatrix}
  (1/\alpha)x^k-\nabla h(x^k)\\
  Ax^k-\alpha \nabla h(x^k)+
  (1/\beta) u ^k-\alpha A A^\intercal u ^k
  \end{bmatrix}.
%   -\begin{bmatrix}
%      \nabla h(x^k) \\
%      \alpha A\nabla h(x^k)
%   \end{bmatrix}.
%      \begin{bmatrix}
%     (1/\alpha)I & A^\intercal  \\
%     0 & (1/\beta)I
%   \end{bmatrix}
%   \begin{bmatrix}
%         x^{k+1} \\
%          u ^{k+1}
%   \end{bmatrix}
%   +
%   \begin{bmatrix}
%   0\\
%   \partial g^*( u ^{k+1})
%   \end{bmatrix}
%   \ni
%   \begin{bmatrix}
%     (1/\alpha)I & 0 \\
%     A & (1/\beta)I - \alpha AA^\intercal
%   \end{bmatrix}
%   \begin{bmatrix}
%         x^{k} \\
%          u ^{k}
%   \end{bmatrix}
%   -\begin{bmatrix}
%      \nabla h(x^k) \\
%      \alpha A\nabla h(x^k)
%   \end{bmatrix}.
\end{align*}
\end{frame}

\begin{frame}
\frametitle{PAPC/PDFP$^2$O}
Compute $ u ^{k+1}$ first and then compute $x^{k+1}$:
\begin{align*}
   u ^{k+1} & = \prox_{\beta g^*}\left(
   u ^k+
  \beta A( x^k - \alpha A^\intercal u ^k-\alpha \nabla h(x^k))\right)\\
  x^{k+1} & = x^k - \alpha A^\intercal  u ^{k+1} -\alpha \nabla h(x^k)
\end{align*}
This is proximal alternating predictor corrector (PAPC)
or primal-dual fixed point algorithm based on proximity operator (PDFP$^2$O).
\vspace{0.2in}

If total duality holds, $h$ is $L$-smooth, $\alpha>0$, $\beta>0$, $\alpha \beta\lambda_{\mathrm{max}}(A^\intercal A)< 1$, and $\alpha<2/L$,
then $x^k\rightarrow x^\star$ and $ u ^k\rightarrow  u ^\star$.

\end{frame}




\begin{frame}[fragile]
\frametitle{Example: Isotonic regression}
Isotonic constraint requires entries of regressor to be nondecreasing.

\vspace{0.2in}

Isotonic regresion with the Huber loss is
\[
\begin{array}{ll}
\underset{x\in \reals^n}{\mbox{minimize}} &
\displaystyle{\ell(Ax-b)}\\
\mbox{subject to} &x_{i+1}-x_i\ge 0\quad\text{for }i=1,\dots,n-1
\end{array}
\]
where $A\in \reals^{m\times n}$, $b\in \reals^m$, and
\[
\ell(y)=\sum^m_{i=1}h(y_i),\qquad
h(r)=\left\{
\begin{array}{ll}
r^2&\text{for } |r|\le 1\\
2|r|-1&\text{for } |r|>1.
\end{array}
\right.
\]

\vspace{0.2in}

What method can we use?
\end{frame}




\begin{frame}[fragile]
\frametitle{Example: Isotonic regression}
The problem is equivalent to
\[
\begin{array}{ll}
\underset{x\in \reals^n}{\mbox{minimize}} &
\displaystyle{
\overbrace{\!\!\!\!\!\!\!\!\!\sum_{i=1,3,\dots,n-1}\!\!\!\!\delta_{\reals_+}(x_{i+1}-x_i)}^{\text{proximable}}
+
\overbrace{\!\!\!\!\!\!\!\!\!\sum_{i=2,4,\dots,n-2}\!\!\!\!\delta_{\reals_+}(x_{i+1}-x_i)}^{\text{proximable}}
+
\overbrace{\ell(Ax-b)}^{\text{differentiable}}}
\end{array}
\]

\vspace{0.2in}

We can use DYS.
\end{frame}




\begin{frame}[fragile]
\frametitle{Example: Isotonic regression}
The problem is equivalent to
\[
\begin{array}{ll}
\underset{x\in \reals^n}{\mbox{minimize}} &
\displaystyle{\ell(Ax-b)+\delta_{\reals_+^{(n-1)}}(Dx)},
\end{array}
\]
where
\[
D=
\begin{bmatrix}
-1&1&0&\cdots&0&0\\
0&-1&1&\cdots&0&0\\
\vdots&&&\ddots&&\vdots\\
0&0&0&\cdots&-1&1\\
\end{bmatrix}\in \reals^{(n-1)\times n}.
\]


\vspace{0.2in}

We can use PAPC.
\end{frame}





\section{Linearization technique}

\begin{frame}[plain]
\frametitle{Linearization technique}
Linearization technique: use a proximal term to cancel out a computationally inconvenient quadratic term.

\vspace{0.2in}
In the update
\[
x^{k+1}=
\argmin_{x\in\reals^n}\left\{
f(x)+\frac{\alpha}{2}\|Ax-b\|^2
+\frac{1}{2}\|x-x^k\|^2_M
\right\}.
\]
If $f$ is proximable, choose $M = \frac{1}{\beta}I - \alpha A^\intercal A$ (with $\frac{1}{\beta}>\alpha\lambda_\mathrm{max}(A^\intercal A)$):
\begin{align*}
f(x)+&\frac{\alpha}{2}\|Ax-b\|^2
+\frac{1}{2}\|x-x^k\|^2_M\\
&=
f(x)
-\alpha \langle Ax,b\rangle
-x^\intercal Mx^k
+\frac{\alpha}{2}x^\intercal A^\intercal Ax
+\frac{1}{2}x^\intercal Mx
+\text{constant}
\\
&= f(x)+\alpha\langle Ax^k-b,Ax\rangle
-\frac{1}{\beta}\langle x^k,x\rangle
+\frac{1}{2\beta}\|x\|^2
+\text{constant}
\\
&= f(x)+\alpha\langle Ax^k-b,Ax\rangle
+\frac{1}{2\beta}\|x-x^k\|^2
+\text{constant}
\\
&=
f(x)
+\frac{1}{2\beta}\left\|x-\left(x^k-
\alpha \beta A^\intercal(Ax^k-b)
\right)
\right\|^2
+\text{constant}
\end{align*}

\end{frame}



\begin{frame}
\frametitle{Linearization technique}
and we have
\[
x^{k+1}
=\prox_{\beta f}\left(
x^{k}-\alpha\beta A^\intercal(Ax^k-b)
\right)
\]
\vspace{0.2in}

Carefully choose $M$ of the ``proximal term'' $\|x-x^k\|^2_M$ to cancel out the quadratic term $x^\intercal A^\intercal Ax$ originating from $\|Ax-b\|^2$.
\vspace{0.2in}

This is as if we linearized the quadratic term
\[
\frac{\alpha}{2}\|Ax-b\|^2\approx
\alpha \langle Ax,Ax^k-b\rangle %+\mathcal{O}(\|Ax^k-b\|^2)
+
\text{constant}
\]
and added $(2\beta)^{-1}\|x-x^k\|^2$ to ensure convergence.
\end{frame}

\begin{frame}
\frametitle{Linearized method of multipliers}
Consider
\[
\begin{array}{ll}
\underset{x\in \reals^n}{\mbox{minimize}} &f(x)\\
\mbox{subject to} &Ax=b.
\end{array}
\]
Let $M\succ 0$ and $K=\alpha^{-1/2}M^{-1/2}$. Re-parameterize with $x=Ky$:
\[
\begin{array}{ll}
\underset{y\in \reals^n}{\mbox{minimize}} &f(Ky)\\
\mbox{subject to} &AKy=b.
\end{array}
\]

Proximal method of multipliers with re-parameterized problem:
\begin{align*}
y^{k+1} &= \argmin_y \left\{
f(Ky)+\langle  u ^k,AKy\rangle+\frac{\alpha}{2}\|AKy-b\|^2
+\frac{1}{2\alpha}\|y-y^k\|^2\right\}\\
 u ^{k+1} &=  u ^k+\alpha (AKy^{k+1}-b)
\end{align*}
\end{frame}



\begin{frame}
\frametitle{Linearized method of multipliers}

Substitute back $x=Ky$:
\begin{align*}
x^{k+1} &= \argmin_x \left\{
f(x)+\langle  u ^k,Ax\rangle+\frac{\alpha}{2}\|Ax-b\|^2
+\frac{1}{2}\|x-x^k\|^2_M\right\}\\
 u ^{k+1} &=  u ^k+\alpha (Ax^{k+1}-b).
\end{align*}
%We call $(1/2)\|x-x^k\|^2_M$ term a \emph{proximal term}.
% which we can write as
% \begin{align*}
% x^{k+1} &= \argmin_x \left\{K_{\alpha} (x, u ^k)
% +(1/2)\|x-x^k\|^2_M\right\}\\
%  u ^{k+1} &=  u ^k+\alpha (Ax^{k+1}-b).
% \end{align*}
% Then we have
% \begin{align*}
% x^{k+1} &= \argmin_y \left\{
% f(x)+\langle  u ^k,Ax-b\rangle
% +\alpha \langle Ax,Ax^k-b\rangle
% +\frac{1}{2\beta}\|x-x^k\|^2\right\}\\
%  u ^{k+1} &=  u ^k+\alpha (Ax^{k+1}-b).
% \end{align*}
Let $M = (1/\beta) I - \alpha A^\intercal A$,
where $\alpha\beta\lambda_\mathrm{max}(A^\intercal A)<1$ so that $M\succ 0$:
\begin{align*}
x^{k+1} &= \argmin_x \left\{
f(x)+\langle  u ^k+\alpha(Ax^k-b),Ax\rangle
+\frac{1}{2\beta}\|x-x^k\|^2\right\}\\
 u ^{k+1} &=  u ^k+\alpha (Ax^{k+1}-b)
\end{align*}

\end{frame}



\begin{frame}
\frametitle{Linearized method of multipliers}
Finally:
\begin{align*}
x^{k+1} &= \prox_{\beta f}\left(x^k-\beta A^\intercal( u ^k+\alpha(Ax^k-b))\right)\\
 u ^{k+1} &=  u ^k+\alpha (Ax^{k+1}-b)
\end{align*}
This is linearized method of multipliers.
\vspace{0.2in}

If total duality holds, $\alpha>0$, $\beta>0$, and $\alpha\beta\lambda_\mathrm{max}(A^\intercal A)<1$,
then $x^k\rightarrow x^\star$ and $ u ^k\rightarrow  u ^\star$.

\vspace{0.2in}
When $\prox_{\beta f}$ is easy to evaluate,
but %$\argmin \lagrange_\alpha$
$\argmin_x\{f(x)+\frac{1}{2}\|Ax-b\|^2\}$
is not, the linearized method of multipliers is useful.
% can be much more effective than the (original) method of multipiers.
\end{frame}




\section{BCV technique}


\begin{frame}
\frametitle{BCV technique}
In the linearization technique, the proximal term $(1/2)\|x-x^k\|^2_M$ must come from somewhere.
%Sometimes we can use methods that already have a proximal term, such as the  proximal method of multipliers or the proximal ADMM of Exercise \ref{exercise:proximal-admm-kkt}.

\vspace{0.2in}
The BCV technique creates proximal terms.


\vspace{0.2in}
(BCV = Bertsekas, O'Connor, and Vandenberghe)

\end{frame}



\begin{frame}
\frametitle{PDHG}
Consider 
\[
\begin{array}{ll}
\underset{x\in \reals^n}{\mbox{minimize}}&f(x)+g(Ax)
\end{array}
\]
%discussed in  Example~\ref{example:fenchel-rockafellar}.

\vspace{0.2in}
Use BCV technique to get equivalent problem
\[
\begin{array}{ll}
\underset{x\in \reals^n,\,\tilde{x}\in \reals^m}{\mbox{minimize}}&
\underbrace{f(x)+
\delta_{\{0\}}(\tilde{x})}_{=\tilde{f}(x,\tilde{x})}
+\underbrace{g(Ax+M^{1/2}\tilde{x})}_{=\tilde{g}(x,\tilde{x})},
\end{array}
\]
for any  $M\succeq 0$.
\vspace{0.2in}


 \end{frame}

 \begin{frame}[plain]
 \frametitle{PDHG}
Consider DRS
\[
(z^{k+1},\tilde{z}^{k+1})=\left(\frac{1}{2}\opI+\frac{1}{2}\opR_{\alpha \partial \tilde{g}}\opR_{\alpha \partial \tilde{f}}\right)
(z^{k},\tilde{z}^{k}).
\]
The identity
\begin{align*}
\label{eq:J_conj}
v=\prox_{\alpha h(B \cdot)}(u)
\quad\Leftrightarrow\quad
\begin{array}{l}
x\in\argmin_x\left\{
h^*(x)-\langle u,B^\intercal x\rangle +\frac{\alpha}{2}\|B^\intercal x\|^2
\right\}\\
v= u-\alpha B^\intercal x,
\end{array}
\end{align*}
becomes 
\begin{align*}
\prox_{\alpha \tilde{g}}(x,\tilde{x})&=(y,\tilde{y})\\
\quad\Leftrightarrow\quad
 u &\in \argmin_{ u }\left\{g^*( u )-\left<
\begin{bmatrix}
x\\\tilde{x}
\end{bmatrix}
,
\begin{bmatrix}
A^\intercal\\M^{1/2}
\end{bmatrix}
 u \right>
+
\frac{\alpha}{2}\left\|\begin{bmatrix}
A^\intercal\\M^{1/2}
\end{bmatrix} u \right\|^2
\right\}\\
y&=x-\alpha A^\intercal u \\
\tilde{y}&=\tilde{x}-\alpha M^{-1/2} u 
\end{align*}
under the regularity condition $\relint\dom g\cap\mathcal{R}([A\,M^{1/2}])\ne \emptyset$.
 \end{frame}

 \begin{frame}
 \frametitle{PDHG}
The FPI:
\begin{align*}
    x^{k+1/2}&=\argmin_{x}\left\{    f(x)+\frac{1}{2\alpha}\|x-z^k\|^2    \right\}\\
    \tilde{x}^{k+1/2}&= 0\\
     u ^{k+1}&=\argmin_{ u }\bigg\{
    g^*( u )-\langle A(2x^{k+1/2}-z^k)-M^{1/2}\tilde{z}^k, u \rangle\\
    &\qquad\qquad\qquad\qquad\qquad+\frac{\alpha}{2}\left(\|A^\intercal u \|^2+\|M^{1/2} u \|^2\right)
    \bigg\}
    \\
    x^{k+1}&=2x^{k+1/2}-z^k-\alpha A^\intercal u ^{k+1}\\
    \tilde{x}^{k+1}&=-\tilde{z}^k-\alpha M^{1/2} u ^{k+1}\\
    z^{k+1}&=x^{k+1/2}-\alpha A^\intercal u ^{k+1}\\
    \tilde{z}^{k+1}&=-\alpha M^{1/2} u ^{k+1}
\end{align*}
 \end{frame}

 \begin{frame}
  \frametitle{PDHG}
Simplify further:
\begin{align*}
    x^{k+1/2}&=\argmin_{x}\left\{    f(x)+\frac{1}{2\alpha}\|x-(x^{k-1/2}-\alpha A^\intercal u ^{k})\|^2    \right\}\\
     u ^{k+1}&=\argmin_{ u }\left\{
    g^*( u )-\langle A(2x^{k+1/2}-x^{k-1/2}), u \rangle
    +\frac{\alpha}{2}\| u - u ^k\|^2_{(AA^\intercal+M)}
    \right\}
\end{align*}
\vspace{0.2in}

Linearize with $M=\frac{1}{\beta\alpha}I-AA^\intercal$, with $\alpha\beta\lambda_\mathrm{max}(A^\intercal A)\le 1$ so $M\succeq 0$:
\begin{align*}
    x^{k+1/2}&=\prox_{\alpha f}(x^{k-1/2}-\alpha A^\intercal u ^{k})\\
     u ^{k+1}&=\prox_{\beta g^*}( u ^k+\beta A(2x^{k+1/2}-x^{k-1/2})).
\end{align*}
If total duality, regularity condition $\relint\dom g\cap\mathcal{R}([A\,M^{1/2}])\ne \emptyset$,
$\alpha>0$, $\beta>0$, and $\alpha\beta\lambda_\mathrm{max}(A^\intercal A)\le 1$ hold, then $x^{k+1/2}\rightarrow x^\star$.
%and $ u ^k\rightarrow  u ^\star$.
 \end{frame}

 \begin{frame}
\frametitle{Convergence analysis: PDHG}
The Lagrangian
\[
\tilde{\lagrange}(x,\tilde{x},\mu,\tilde{\mu})=
g(Ax+M^{-1/2}\tilde{x})
+\langle x,\mu\rangle
+\langle \tilde{x},\tilde{\mu}\rangle
-f^*(\mu)
\]
generates the stated equivalent primal problem
and the dual problem
\[
\begin{array}{ll}
\underset{\mu\in \reals^n,\,\tilde{\mu}\in \reals^{m}}{\mbox{maximize}}&
-\left(
\begin{bmatrix}
A^\intercal\\M^{1/2}
\end{bmatrix}
\rhd
g^*\right)(-\mu,-\tilde{\mu})
-f^*(\mu)
\end{array}
\]

\vspace{0.2in}
If the original primal-dual problems of page~\pageref{frame_pdhg1} has solutions $x^\star$ and $ u ^\star$ for which strong duality holds,
then the equivalent problems have solutions $(x^\star,0)$ and
$(-A^\intercal u ^\star,-M^{1/2} u ^\star)$ for which strong duality holds.
I.e., [total duality original problem] $\Rightarrow$ [total duality equivalent problem]

\vspace{0.2in}
So DRS converges under the stated assumptions.
 \end{frame}


\begin{frame}[plain]
\frametitle{PD3O}
Consider 
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^n}}{\mbox{minimize}}&f(x)+h(x)+g(Ax)
\end{array}
\]

\vspace{0.2in}

Use BCV technique to get the equivalent problem
\[
\begin{array}{ll}
\underset{x\in \reals^n,\,\tilde{x}\in \reals^m}{\mbox{minimize}}&
\underbrace{f(x)+
\delta_{\{0\}}(\tilde{x})}_{=\tilde{f}(x,\tilde{x})}
+\underbrace{g(Ax+M^{1/2}\tilde{x})}_{=\tilde{g}(x,\tilde{x})}
+\underbrace{h(x)}_{=\tilde{h}(x,\tilde{x})}
\end{array}
\]

The DYS FPI
\[
(z^{k+1},\tilde{z}^{k+1})=
(\opI-\opJ_{\alpha \partial \tilde{f}}+\opJ_{\alpha \partial \tilde{g}}(\opR_{\alpha \partial \tilde{f}}-\alpha \nabla\tilde{h}\opJ_{\alpha \partial \tilde{f}}))
(z^{k},\tilde{z}^{k})
\]
with $M=(\beta\alpha)^{-1}I-AA^\intercal$:
\begin{align*}
    x^{k+1}&=\prox_{\alpha f}\left(x^{k}-\alpha A^\intercal u ^{k}-\alpha \nabla h(x^{k})\right)\\
     u^{k+1}&=\prox_{\beta g^*}\left(
     u^k+\beta A\left(2x^{k+1}-x^{k} + \alpha \nabla h(x^{k}) - \alpha \nabla h(x^{k+1})\right)
    \right).
    % I switched \nabla h(x^{k+1}) and \nabla h(x^{k}). Wotao
\end{align*}

This is primal-dual three-operator splitting (PD3O).
%If total duality holds, $\alpha>0$, $\beta>0$, $\alpha\beta\lambda_\mathrm{max}(A^\intercal A)\le 1$, and $\alpha<L/2$, then $x^{k+1/2}\rightarrow x^\star$.
\end{frame} 


\begin{frame}
\frametitle{Condat--V\~u vs.\ PD3O}
Condat--V\~u and PD3O  solve 
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^n}}{\mbox{minimize}}&f(x)+h(x)+g(Ax).
\end{array}
\]
Condat--V\~u generalizes PDHG.
PD3O generalizes PAPC and PDHG.
%The two methods are very similar when compared side-by-side
%and have essentially identical computational costs per iteration.

\vspace{0.2in}

Condat--V\~u:
\begin{align*}
x^{k+1}&=
\prox_{\alpha f}(x^k-\alpha A^\intercal u ^k-\alpha \nabla h(x^k))\\
 u ^{k+1}&=
\prox_{\beta g^*}( u ^k+\beta A(2x^{k+1}-x^k))
\end{align*}


PD3O:
\begin{align*}
    x^{k+1}&=\prox_{\alpha f}\left(x^{k}-\alpha A^\intercal u ^{k}-\alpha \nabla h(x^{k})\right)\\
     u^{k+1}&=\prox_{\beta g^*}\left(     u^k+\beta A\left(2x^{k+1}-x^{k} + {\color{red}\alpha \nabla h(x^{k}) - \alpha \nabla h(x^{k+1})}\right)
    \right)
\end{align*}
\end{frame}





\begin{frame}
\frametitle{Condat--V\~u vs.\ PD3O}
Convergence criterion slightly differ.

\vspace{0.2in}
Condat--V\~u:
\[\alpha\beta\lambda_\mathrm{max}(A^\intercal A)+\alpha L/2<1\]


PD3O:
\[\alpha\beta\lambda_\mathrm{max}(A^\intercal A)\le 1\text{ and }\alpha L/2<1\]

\vspace{0.2in}

Roughly speaking, PD3O can use stepsizes twice as large.
This can lead to PD3O being twice as fast.
\end{frame}



\begin{frame}
\frametitle{Proximal ADMM}
Consider
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f(x)+g(y)\\
\mbox{subject to}&Ax+By=c.
\end{array}
\]
Let $M\succeq 0$, $N\succeq 0$, $P=\alpha^{-1/2}M^{1/2}$, and $Q=\alpha^{-1/2}N^{1/2}$.

\vspace{0.2in}


Use dual form of the BCV technique to get equivalent problem
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q\\\tilde{x}\in \reals^q,\,\tilde{y}\in \reals^p}}{\mbox{minimize}}&f(x)+g(y)\\
\mbox{subject to}&
\begin{bmatrix}
A&0\\
P&0\\
0&I
\end{bmatrix}
\begin{bmatrix}
x\\
\tilde{x}
\end{bmatrix}+\begin{bmatrix}
B&0\\
0&I\\
Q&0
\end{bmatrix}
\begin{bmatrix}
y\\
\tilde{y}
\end{bmatrix}=
\begin{bmatrix}
c\\
0\\
0
\end{bmatrix}
.
\end{array}
\]
\end{frame}




\begin{frame}
\frametitle{Proximal ADMM}
Apply ADMM:
\begin{align*}
x^{k+1}&\in
 \argmin_{x\in\reals^p} \left\{
 \lagrange_\alpha (x,y^k, u ^k)
 +\langle \tilde{ u }^{k}_1,
 Px
 \rangle
 +
 \frac{\alpha}{2}\|Px+\tilde{y}^k\|^2\right\}
 \\
\tilde{x}^{k+1}&=
 \argmin_{\tilde{x}\in\reals^q} \left\{
 \langle \tilde{ u }^k_2,
 \tilde{x}\rangle+
 \frac{\alpha}{2}\|
 \tilde{x}+Qy^k
 \|^2
 \right\}
 \\
 &=
 -Qy^k-(1/\alpha)\tilde{ u }_2^k
 \\
 y^{k+1}&\in
 \argmin_{y\in\reals^q} \left\{\lagrange_\alpha(x^{k+1},y, u ^k)
 +\langle \tilde{ u }^k_2,Qy\rangle
 +\frac{\alpha}{2}\|\tilde{x}^{k+1}+Qy\|^2
 \right\}
 \\
 \tilde{y}^{k+1}&=
 \argmin_{\tilde{y}\in\reals^p} \left\{
 \langle \tilde{ u }^k_1,
 \tilde{y}\rangle
 +\frac{\alpha}{2}\|Px^{k+1}+\tilde{y}\|^2
 \right\}\\
 &=-Px^{k+1}-(1/\alpha)\tilde{ u }^k_1
 \\
 u ^{k+1}&= u ^k+\alpha(Ax^{k+1}+By^{k+1}-c)\\
\tilde{ u }_1^{k+1}&=\tilde{ u }_1^{k}+\alpha(Px^{k+1}+\tilde{y}^{k+1})=0\\
\tilde{ u }_2^{k+1}&=\tilde{ u }_2^{k}+\alpha(\tilde{x}^{k+1}+Qy^{k+1})
=\alpha Q(y^{k+1}-y^k)
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Proximal ADMM}
Simplify:
\begin{align*}
x^{k+1}&\in
 \argmin_{x} \left\{
 \lagrange_\alpha (x,y^k, u ^k)
 +
 \frac{1}{2}\|x-x^k\|^2_M\right\}
 \\
 y^{k+1}&\in
 \argmin_{y} \left\{\lagrange_\alpha(x^{k+1},y, u ^k)
 +\frac{1}{2}\|y-y^k\|^2_N
 \right\}
  \\
 u ^{k+1}&= u ^k+\alpha(Ax^{k+1}+By^{k+1}-c)
\end{align*}
This is proximal ADMM.

\vspace{0.2in}
If total duality, $M\succeq 0$,  $N\succeq 0$, $(\cR(A^\intercal)+\cR(M))\cap \relint\dom f^*\ne \emptyset$, $(\cR(B^\intercal)+\cR(N))\cap \relint\dom g^*\ne \emptyset$, and $\alpha>0$ hold,
then $ u ^k\rightarrow  u ^\star$,
$Ax^{k}\rightarrow Ax^\star$,
$Mx^{k}\rightarrow Mx^\star$,
$By^{k}\rightarrow By^\star$, and
$Ny^{k}\rightarrow Ny^\star$.

% If furthermore $M\succ 0$, then we can multiply $M^{-1}$ to
% $Mx^{k}\rightarrow Mx^\star$ and conclude
% $x^{k}\rightarrow x^\star$.
% Likewise, if furthermore $N\succ 0$, then
% $y^k\rightarrow y^\star$.
% If $g$ is furthermore strictly convex, then $x^k\rightarrow x^\star$.
% If $f$ is furthermore strictly convex, then and $y^k\rightarrow y^\star$.
\end{frame}




%\begin{frame}
%\frametitle{Convergence analysis.}
%The Lagrangian
%\[
%\lagrange(x,y, u ,\tilde{ u }_1,\tilde{ u }_2)=
%f(x)+g(y)+\langle  u ,Ax+By-c\rangle
%+\langle \tilde{ u }_1,Px+\tilde{y}\rangle
%+\langle \tilde{ u }_2,\tilde{x}+Qy\rangle
%\]
%generates the equivalent primal problem.
%$\lagrange$ generates the dual problem
%\begin{equation*}
%\begin{array}{ll}
%\underset{ u  \in \reals^n}{\mbox{maximize}}&
%\!\!\!\!
%-f^*(-A^\intercal u -P\tilde{ u }_1)
%-\delta_{\{0\}}(-\tilde{ u }_2)
%-g^*(-B^\intercal u -Q\tilde{ u }_2)
%-\delta_{\{0\}}(-\tilde{ u }_1)
%-c^\intercal u 
%\end{array}
%\end{equation*}
%If the original problems
%\eqref{eq:admm-primal} and \eqref{eq:admm-dual}
%have solutions $(x^\star,y^\star)$ and $ u ^\star$ for which strong duality holds,
%then the equivalent problems have solutions
%$(x^\star,y^\star)$ and $( u ^\star,0)$ for which strong duality holds.
%In other words, total duality of the original problems imply total duality of the equivalent problems.
%So under the stated assumptions, ADMM applied to the equivalent problem converges,
%and we get the stated convergence results.
%
%
%
%Finally, note that the equivalent dual problem
%resembles what we had when we applied the BCV technique to PDHG.
%What we did is the BCV technique applied to the dual.
%\end{frame}

\begin{frame}
\frametitle{Linearized ADMM}
Consider 
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f(x)+g(y)\\
\mbox{subject to}&Ax+By=c.
\end{array}
\]


\vspace{0.2in}

Proximal ADMM with $M=\frac{1}{\beta}I-\alpha A^\intercal A$ and $N=\frac{1}{\gamma}I-\alpha B^\intercal B$:
\begin{align*}
x^{k+1}&=\argmin_{x}\left\{
    f(x)+\langle  u ^{k},Ax\rangle+
    \alpha \langle Ax,Ax^k+By^k-c\rangle
    +\frac{1}{2\beta}\|x-x^{k}\|^2
    \right\}\\
y^{k+1}&=\argmin_{y}\left\{
    g(y)+\langle  u ^k,By\rangle +
    \alpha \langle By,Ax^{k+1}+By^k-c\rangle
    +\frac{1 }{2\gamma}\|y-y^{k}\|^2
    \right\}\\
 u ^{k+1}&= u ^{k}+\alpha(Ax^{k+1}+By^{k+1}-c)
\end{align*}
\end{frame}



\begin{frame}
\frametitle{Linearized ADMM}
Simplify:
\begin{align*}
x^{k+1}&=
\prox_{\beta f}\left(
x^k-\beta A^\intercal( u ^k
+
\alpha (Ax^k+By^k-c))\right)\\
y^{k+1}&=
\prox_{\gamma g}
\left(
y^k-\gamma B^\intercal( u ^k+\alpha (Ax^{k+1}+By^k-c) )
\right)\\
 u ^{k+1}&= u ^{k}+\alpha(Ax^{k+1}+By^{k+1}-c)
\end{align*}
This is linearized ADMM.

\vspace{0.2in}
If total duality holds, $\alpha>0$, $\beta>0$, $\gamma>0$,
$\alpha\beta\lambda_\mathrm{max}(A^\intercal A)\le 1$, and $\alpha\gamma\lambda_\mathrm{max}(B^\intercal B)\le 1$
then $x^k\rightarrow x^\star$, $y^k\rightarrow y^\star$, and $ u ^k\rightarrow u ^\star$.
\end{frame}


\begin{frame}[plain]
\frametitle{PDHG}
Consider 
\[
    \begin{array}{ll}
    \underset{y\in \reals^m,\,x\in \reals^n}{\mbox{minimize}}&g(y)+f(x)\\
    \mbox{subject to}&-Iy+Ax=0
    \end{array}
\]
which is equivalent to the problem of page \pageref{frame_pdhg1}.

\vspace{0.2in}

Linearized ADMM:
\begin{align*}
y^{k+1}&=
\prox_{\beta g}
\left(
y^k+\beta ( u ^k-\alpha (y^k-Ax^{k}) )
\right)\\
x^{k+1}&=
\prox_{\gamma f}\left(
x^k-\gamma A^\intercal( u ^k
-
\alpha (y^{k+1}-Ax^{k}))\right)\\
 u ^{k+1}&= u ^{k}-\alpha(y^{k+1}-Ax^{k+1})
\end{align*}
\vspace{0.1in}

Let $\beta=1/\alpha$ and use Moreau identity:
\begin{align*}
y^{k+1}&=(1/\alpha) u ^k+ Ax^{k}
-(1/\alpha)\underbrace{\prox_{\alpha  g^*}\left( u ^k+\alpha Ax^k\right)}_{=\mu^{k+1}}\\[-1em]
%\mu^{k+1}&=  u ^k+\alpha(Ax^k-y^{k+1})\\
x^{k+1}&=\prox_{\gamma f}\left(x^k-\gamma A^\intercal\mu^{k+1}\right)\\
 u ^{k+1}&=\mu^{k+1}+\alpha A(x^{k+1}-x^k)
\end{align*}
\end{frame}



\begin{frame}
\frametitle{PDHG}
Recover PDHG:
\begin{align*}
\mu^{k+1}&=\prox_{\alpha  g^*}
\left(
\mu^k+\alpha A(2x^k-x^{k-1})
\right)\\
x^{k+1}&=
\prox_{\gamma f}\left(
x^k-\gamma A^\intercal\mu^{k+1}\right)
\end{align*}

\vspace{0.2in}

If total duality, $\alpha>0$, $\gamma>0$, $\alpha\gamma\lambda_\mathrm{max}(A^\intercal A)\le1$ hold, then $\mu^k\rightarrow  u ^\star$ and $x^k\rightarrow x^\star$.
\end{frame}


\begin{frame}
\frametitle{Conclusion}
We analyzed convergence of a wide range of splitting methods.
%through reducing them to another method for which we have already established convergence.
\vspace{0.2in}


At a detailed level, the many techniques are not obvious and require many lines of calculations.
At a high level, the approach is to reduce all methods to an FPI and apply Theorem~1.
% with the following unified approach:
%Once we have the machinery established,we derive algorithms with the following unified approach:
% \begin{enumerate}
% \item Transform the given convex optimization problem into an equivalent convex optimization problem.
% This step is optional.
% \item Transform the convex optimization problem into a monotone inclusion problem.
% \item With a splitting, transform the monotone inclusion problem into a fixed-point problem.
% \item Find a fixed point using a fixed-point iteration.
% \end{enumerate}
% This derivation will provide a convergence analysis.
\vspace{0.2in}

Given an optimization problem, which method do we choose?\\
In practice, a given problem usually has at most a few methods that apply conveniently.
A good rule of thumb is to first consider methods with a low per-iteration cost.

\end{frame}

\iffalse
\fi
\end{document}
