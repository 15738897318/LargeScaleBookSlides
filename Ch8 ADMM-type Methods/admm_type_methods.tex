\documentclass[10pt,mathserif]{beamer}

\input{../preambles/defs.tex}
\input{../preambles/beamer_setting}
\input{../preambles/affiliation}

\title{\large \bfseries ADMM-Type Methods}

\begin{document}

\frame{
\thispagestyle{empty}
\titlepage
}

\begin{frame}
\frametitle{Overview}
%ADMM-type methods refer to the alternating direction method of multipliers (ADMM) and its variants.
%\bigskip

This lecture covers
\smallskip
\begin{witemize}
  \item FLiP-ADMM, a general and highly versatile variant of ADMM
  \item Convergence analysis of FLiP-ADMM
  \item A wide range of ADMM-type methods derived from FLiP-ADMM
  \item Further generalization to Trip-ADMM and discussions
\end{witemize}
\end{frame}

\section{FLiP-ADMM method, main theorem, discussions}

\begin{frame}{Function-Linearized Proximal ADMM (FLiP-ADMM)}
Consider primal problem
\begin{equation}\label{eq:FlpADMM}
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&\underbrace{f_1(x)+f_2(x)}_{f(x)}+\underbrace{g_1(y)+g_2(y)}_{g(x)}\\
\mbox{subject to}&Ax+By=c,
\end{array}
\end{equation}
generated by 
\[
\lagrange(x,y,u )=f(x)+g(y)+\langle u , Ax+By-c\rangle.
\]

Assume $f_1,f_2,g_1,g_2$ are CCP and $f_2,g_2$ are also differentiable.

\end{frame}

\begin{frame}
Choose 
\begin{itemize}
  \item scalars $\rho,\varphi>0$
  \item matrices $P\in \reals^{p\times p}$ and $Q\in \reals^{q\times q}$ obeying $P,Q\succeq 0$.
\end{itemize}
\medskip\pause

FLiP-ADMM is the method
\begin{align*}
 x^{k+1} &\in \argmin_{x\in \reals^p} \Big\{f_1(x) + \langle \nabla f_2(x^k) + A^\intercal  u^k, x\rangle \\
 &\hspace{50pt} + \frac{\rho}{2}\|Ax+By^k-c\|^2 + \frac{1}{2}\|x-x^k\|_P^2\Big\}\\
 y^{k+1} &\in \argmin_{y\in \reals^q} \Big\{g_1(y) + \langle \nabla g_2(y^k) + B^\intercal  u^k, y\rangle \\
 &\hspace{50pt} + \frac{\rho}{2}\|Ax^{k+1}+By-c\|^2 + \frac{1}{2}\|y-y^k\|_Q^2\Big\}\\
 u^{k+1} &= u^k + \varphi\rho(Ax^{k+1}+By^{k+1}-c).
\end{align*}
\medskip

Recover classic ADMM with $f_2=0$, $g_2=0$, $P=0$, $Q=0$, and $\varphi = 1$.
\end{frame}

\begin{frame}{Convergence theorem}
\setcounter{theorem}{5}
\begin{theorem}
\label{thm:FLiP-ADMM}
Assume total duality, that $x$- and $y$-subproblems always have solutions,
that $f_2$ is $L_f$-smooth and $g_2$ is $L_g$-smooth,
and there is an $\varepsilon\in(0,2-\varphi)$ such that
\[
P \succeq L_f I,
\qquad
Q\succeq 0,
\qquad
 \rho\left(1-\frac{(1-\varphi)^2}{2-\varphi-\varepsilon}\right) B^\intercal  B + Q \succeq  3L_g I.
\]
Then FLiP-ADMM iterates $x^k,y^k$ satisfy
\[
f(x^k)+g(y^k)\rightarrow f(x^\star)+g(y^\star),\qquad
Ax^k+By^k-c\rightarrow 0,
\]
where $(x^\star,y^\star)$ is a solution of the primal problem.
\end{theorem}
\end{frame}

\begin{frame}
The condition 
\begin{equation}
 \rho\left(1-\frac{(1-\varphi)^2}{2-\varphi-\varepsilon}\right) B^\intercal  B + Q \succeq  3L_g I
 \label{eq:rho_constraint}
\end{equation}
imposes restrictions on $\varphi,\rho$:
since $\varphi=\frac{\sqrt{5}+1}{2}$ leads to $1-\frac{(1-\varphi)^2}{2-\varphi}=0$,
\begin{itemize}
  \item if $\varphi\in(0,\frac{\sqrt{5}+1}{2})$, then $\exists$ small $\varepsilon$ such that $1-\frac{(1-\varphi)^2}{2-\varphi-\varepsilon}>0$, so large $\rho$ helps to meet \eqref{eq:rho_constraint};
  \item if $\varphi\in(\frac{\sqrt{5}+1}{2},2)$ and $\varepsilon\in(0,2-\varphi)$, then $1-\frac{(1-\varphi)^2}{2-\varphi-\varepsilon}<0$, so small $\rho$ helps to meet \eqref{eq:rho_constraint}.
\end{itemize}

\pause\medskip

Besides appearing in the conditions, the choices of LiP-ADMM parameters affect convergence speed and computational cost per iteration.
The optimal choice for a given problem balances the speed and the cost.
\end{frame}

\section{Discussions of parameter choices, special cases, and differences}

\begin{frame}{Golden-ratio ADMM, Dual extrapolation parameter $\varphi$}
While $\varphi=1$ is common, a larger $\varphi$ may provide a speedup.
\medskip

With $f_2=0$, $g_2=0$, $P=0$, and $Q=0$, FLiP-ADMM reduces to ``Golden-ratio ADMM'':
\begin{align*}
x^{k+1}&\in
 \argmin_{x\in \mathbb{R}^p}  \lagrange_\rho(x,y^k,u^k)\\
 y^{k+1}&\in
 \argmin_{y\in \mathbb{R}^q}\lagrange_\rho(x^{k+1},y,u^k) \\
u^{k+1}&=u^k+\varphi\rho(Ax^{k+1}+By^{k+1}-c),
\end{align*}
where
\[
\lagrange_\rho (x,y,u )=f(x)+g(y)+\langle u , Ax+By-c\rangle+\frac{\rho}{2}\|Ax+By-c\|^2.
\]
Condition \eqref{eq:rho_constraint} reduces to $0<\varphi < (1+\sqrt{5})/2\approx 1.618$.
\end{frame}

\begin{frame}{Penalty parameter $\rho$}
Parameter $\rho$ controls the relative priority between primal and dual convergence.
\bigskip

The Lyapunov function in the proof (below) contains the terms
\begin{itemize}
  \item primal error: $\rho\|B(y^k-y^\star)\|^2$,
  \item dual error: $\frac{1}{\varphi\rho} \|u^k-u^\star\|^2$.
\end{itemize}
\bigskip

Large $\rho$ prioritizes primal accuracy while small $\rho$ prioritizes dual accuracy.
%When $0<\varphi<\frac{\sqrt{5}+1}{2}$, we can use large $\rho$.
%When $\frac{\sqrt{5}+1}{2}\le \varphi<2$, we can use small $\rho$.
\end{frame}


\begin{frame}{Proximal terms via $P$ and $Q$}
The letter ``P'' in FLi\textbf{P}-ADMM describes the presence of the \emph{proximal terms}
\[
\frac{1}{2}\|x-x^k\|_P^2,\qquad
\frac{1}{2}\|y-y^k\|_Q^2.
\]
\medskip

Empirically, smaller $P$ and $Q$ leads to fewer required iterations.
\bigskip

When $f_2=0$ and $g_2=0$, the choice $P=0$ and $Q=0$ is often optimal in the number of required iterations.
\bigskip

However, proper choices of $P$ and $Q$ can cancel out unwieldy quadratic terms and thus reduce the costs of subproblems.

\end{frame}

\begin{frame}{Linearization of $f_2,g_2$}
The $x$-subproblem of FLiP-ADMM
\begin{align*}
x^{k+1} \in \argmin_{x\in \reals^p} \bigg\{&f_1(x) +
f_2(x^k)+\langle \nabla f_2(x^k) , x-x^k\rangle +g(y^k)\\
&+\langle u^k, Ax+By^k-c\rangle + \frac{\rho}{2}\|Ax+By^k-c\|^2 + \frac{1}{2}\|x-x^k\|_P^2\bigg\},
\end{align*}
uses $f_2$'s first-order approximation 
$f_2(x^k)+\langle \nabla f_2(x^k) , x-x^k\rangle,$
described by ``FLi (Function-Linearized)'' in \textbf{FLi}P-ADMM.
\bigskip

FLiP-ADMM gives us the choice to use $f_2$ or not. Choosing $f_2=0$ leads to fewer iterations. In some cases, however, nonzero $f_2$ reduces the cost of subproblem.
\bigskip

The same discussion holds for the $y$-subproblem.
\end{frame}

\begin{frame}{Relation to Method of Multipliers}
`MM'' in FLiP-AD\textbf{MM} stands for \emph{method of multipliers}, which has only one primal subproblem.
\bigskip

When $q=0$, the entire $y$-subproblem and $B$-matrix vanish. FLiP-ADMM reduces to the method of multipliers:
\begin{align*}
x^{k+1}&\in
 \argmin_{x} \left\{f(x) + \langle u^k, Ax\rangle +\frac{\rho}{2}\| Ax-c\|^2\right\}\\
u^{k+1}&=u^k+\varphi\rho(Ax^{k+1}-c),
\end{align*}
which converges for $\varphi\in(0,2)$ by Theorem~\ref{thm:FLiP-ADMM}.
\end{frame}

\section{Proof of main theorem}

\begin{frame}{Difference from previous lectures}

Theorem~\ref{thm:FLiP-ADMM} establishes:
\begin{itemize}
  \item the convergence of objective values,
  \item the convergence of constraint violations,
\end{itemize}
but not the convergence of iterates.
\bigskip

The convergence proof (below) does not rely on the machinery of monotone operators.
\end{frame}

\begin{frame}{About the proof}
The key challenge is the construction of the \emph{Lyapunov function} (a name borrowed from nonlinear system, used to prove the system's stability).
\bigskip

The proof is not long (only 4 pages in the textbook), easy to follow, but hardly intuitive.
\bigskip

ADMM-type methods are modular. %, and classic ADMM is the simplest. %The proof for a simple ADMM-type method are the basis of those for a more general method.
%\bigskip
Hence, the proof comes from the insights we accumulated over years of reading (and writing) papers on ADMM-type methods.
\end{frame}

\begin{frame}{Constants and Lyapunov function}
The assumption of total duality means $\lagrange$ has a saddle point $(x^\star,y^\star,u^\star)$. Define
  \begin{align*}
    w^\star = \begin{bmatrix}
            x^\star\\
            y^\star\\
            u^\star
          \end{bmatrix}
    ,\qquad
    w^k = \begin{bmatrix}
            x^k\\
            y^k\\
            u^k
          \end{bmatrix}
          \quad\text{ for }k=0,1,\dots.
  \end{align*}
  Define $\eta=2-\varphi-\varepsilon$.
  Define the symmetric positive semidefinite matrices
  \begin{gather*}
      M_0 = \frac{1}{2}
        \begin{bmatrix}
          P & 0 & 0\\
          0 & \rho B^\intercal  B + Q & 0\\
          0 & 0 & \frac{1}{\varphi\rho} I
        \end{bmatrix},\qquad
  M_1 = \frac{1}{2}
      \begin{bmatrix}
        0 & 0 & 0 \\
        0 & Q + L_g I  & 0 \\
        0 & 0 & \frac{\eta}{\varphi^2\rho}I
      \end{bmatrix},
    \\
  M_2 = \frac{1}{2}
      \begin{bmatrix}
        P-L_f I & 0 & 0 \\
        0 & \rho\left(1-\frac{(1-\varphi)^2}{\eta}\right) B^\intercal  B + Q - 3L_g I& 0 \\
        0 & 0 & \frac{2-\varphi -\eta}{\varphi^2\rho}I
      \end{bmatrix}.
  \end{gather*}
Define the Lyapunov function
\[
V^k= \|w^k-w^\star\|_{M_0}^2 + \|w^{k}-w^{k-1}\|_{M_1}^2.
\]
\end{frame}


\begin{frame}{Proof sketch}
The proof has 4 stages. We present only the key terms. You should focus on the proof flow rather than the each single term. %(The details only make sense in the full proof.)
\bigskip

\textbf{Stage 1:} Use the facts that $x^{k+1}$ and $y^{k+1}$ are subproblem minimizers to obtain inequalities that relate $x^{k+1}$ with $x^\star$ and $y^{k+1}$ with $y^\star$. Add those inequalities and combine terms to arrive at:
\begin{align}
%\begin{split}
&\lagrange(x^{k+1},y^{k+1},u^\star)-\lagrange(x^\star,y^\star,u^\star)   \label{eq:extended-admm-step1}
\\
&\le
\frac{L_f}{2}\|x^{k+1}-x^k\|^2+\frac{L_g}{2}\|y^{k+1}-y^k\|^2
+\left(1-\frac{1}{\varphi}\right)\frac{1}{\varphi\rho}\|u^{k+1}-u^k\|^2 \nonumber\\
&
 \quad-2\langle
w^{k+1}-w^{k},w^{k+1}-w^\star
\rangle_{M_0}
+\frac{1}{\varphi}\langle u^{k+1}-u^k,B(y^{k+1}-y^k)\rangle. \nonumber
%\end{split}
\end{align} 
Since we cannot determine the signs of the two inner-product terms, we must transform them.
\end{frame}

\begin{frame}

\textbf{Stage 2:} Bound $\frac{1}{\varphi}\langle u^{k+1}-u^k,B(y^{k+1}-y^k)\rangle$. 
\medskip

Use the fact that $y^{k},y^{k+1}$ are minimizers to their respective subproblems to obtain inequalities that relate them. Add those inequalities to get
\begin{align*}
  & \frac{1}{\varphi}\langle u^{k+1}-u^k,B (y^{k+1}-y^k) \rangle  \\
  & \le \frac{L_g}{2} \|y^{k+1}-y^k\|^2 + \frac{L_g}{2}\|y^k-y^{k-1}\|^2
      - \|y^{k+1}-y^k\|_Q^2\\
      &\quad
      + \langle y^{k+1}-y^k, y^k-y^{k-1} \rangle_Q - \left(1-\frac{1}{\varphi}\right)\langle u^k-u^{k-1},B (y^{k+1}-y^k) \rangle.
\end{align*}
Apply Young's inequality $
  \langle  a, b\rangle \le \frac{\zeta}{2}\|a\|^2 + \frac{1}{2\zeta}\|b\|^2$ to last 2 terms ...
\end{frame}

\begin{frame}
... to get
\begin{align}
&\frac{1}{\varphi}\langle u^{k+1}-u^k,B(y^{k+1}-y^k)\rangle\le
\frac{1}{2}\|y^{k+1}-y^k\|_{L_gI-Q+\frac{(1-\varphi)^2}{\eta}\rho B^\intercal  B}^2 \label{eq:extended-admm-step3-mid}\\
&\hspace{50pt}+\frac{1}{2}\|y^k-y^{k-1}\|^2_{L_gI+Q}
+\frac{\eta}{2\varphi^2\rho}\|u^k-u^{k-1}\|^2
\nonumber
\end{align}  
\medskip

If we had applied Young's inequality to $\frac{1}{\varphi}\langle u^{k+1}-u^k,B(y^{k+1}-y^k)\rangle$ directly, then we couldn't get $\|y^{k}-y^{k-1}\|^2$ and $\|u^{k}-u^{k-1}\|^2$ terms and thus not $V^k$ (which is easy to try and verify).
\end{frame}

\begin{frame}
\textbf{Stage 3:} Substitute \eqref{eq:extended-admm-step3-mid} and the generalized cosine identity
\[
\|w^{k+1}-w^\star\|^2_{M_0}=
\|w^{k}-w^\star\|^2_{M_0}-
\|w^{k+1}-w^{k}\|^2_{M_0}
+
2\langle w^{k+1}-w^k,w^{k+1}-w^\star\rangle_{M_0}
\]
into \eqref{eq:extended-admm-step1}; after combine terms, we arrive at the master inequality
\begin{align*}
V^{k+1}&\le V^k
-\|w^{k+1}-w^k\|^2_{M_2}-\left(\lagrange(x^{k+1},y^{k+1},u^\star)-\lagrange(x^\star,y^\star,u^\star)\right).
\end{align*}
\medskip

Since $(x^\star,y^\star,u^\star)$ is a saddle point of $\lagrange$, 
  \[
\lagrange(x^{k+1},y^{k+1},u^\star)-\lagrange(x^\star,y^\star,u^\star)\ge 0.
\]
\end{frame}

\begin{frame}
\textbf{Stage 4:} Applying the summability argument on the master inequality tells us
\begin{witemize}
\item $\|w^{k+1}-w^k\|^2_{M_2}\rightarrow 0$, from which we conclude $u^{k+1}-u^k\rightarrow 0$ and thus \[ Ax^k+Bx^k-c\rightarrow 0;\]
\item $\lagrange(x^{k+1},y^{k+1},u^\star)-\lagrange(x^\star,y^\star,u^\star)\rightarrow 0$, from which and
\[
\lagrange(x^{k+1},y^{k+1},u^\star)=
f(x^{k+1})+g(y^{k+1})+
\underbrace{\langle u^\star,Ax^{k+1}+By^{k+1}-c\rangle}_{\rightarrow 0},
\]
we also conclude \[f(x^{k})+g(y^{k})\rightarrow f(x^{\star})+g(y^{\star}).\]
\end{witemize}
\end{frame}

\section{Derived ADMM-type methods}

\begin{frame}{Linearized methods}
``Linearization'' refers to more than one technique.
Most often, it refers to canceling out inconvenient quadratic terms, leaving with linear terms.
\medskip

Consider
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f_1(x)+g_1(y)\\
\mbox{subject to}&Ax+By=c,
\end{array}
\]
where $f_2=0$ and $g_2=0$.
\medskip

With $P=(1/\alpha)I-\rho A^\intercal  A$ and $Q=(1/\beta)I-\rho B^\intercal  B$, we \textbf{recover linearized ADMM} (we saw this method in CH3 with $\varphi=1$):
\begin{align*}
x^{k+1}&=
\prox_{\alpha f}\left(x^k-\alpha A^\intercal ( u ^k+\rho (Ax^k+By^k-c))\right)\\
y^{k+1}&=
\prox_{\beta g}\left(y^k-\beta B^\intercal ( u ^k+\rho (Ax^{k+1}+By^k-c) )\right)\\
 u ^{k+1}&= u ^{k}+\varphi\rho(Ax^{k+1}+By^{k+1}-c).
\end{align*}
Converge if $1\ge \alpha\rho\lambda_\mathrm{max}(A^\intercal  A)$, $1\ge \beta\rho\lambda_\mathrm{max}(B^\intercal  B)$, $\varphi<(1+\sqrt{5})/2$.
\end{frame}

\begin{frame}

Consider
\[
    \begin{array}{ll}
    \underset{x\in \reals^p,\,y\in \reals^q}{\mbox{minimize}}&f_1(x)+g_1(y)\\
    \mbox{subject to}&-Ix+By=0.
    \end{array}
\]
\medskip

We \textbf{recover primal-dual hybrid gradient (PDHG)} with $\varphi=1$, $P=0$, $Q=(1/\beta)I-\rho B^\intercal  B$ in an FLiP-ADMM:
\begin{align*}
\mu^{k+1}&=\prox_{\rho  f^*_1}\left(\mu^k+\rho B(2y^k-y^{k-1})\right)\\
y^{k+1}&=\prox_{\beta g_1}\left(y^k-\beta B^\intercal \mu^{k+1}\right).
\end{align*}

Converge if $1\ge \beta \rho\lambda_\mathrm{max}(B^\intercal  B)$.
\end{frame}

\begin{frame}{Function-linearized methods}
FLiP-ADMM linearizes accesses $f_2$ and $g_2$ through their gradient evaluations.
This feature provides great flexibility.
\medskip

Consider
\[
    \begin{array}{ll}
    \underset{x\in \reals^p,\,y\in \reals^q}{\mbox{minimize}}&f_1(x)+g_1(y)+g_2(y)\\
    \mbox{subject to}&-Ix+By=0.
    \end{array}
\]
FLiP-ADMM with $\varphi=1$, $P=0$, and $Q=(1/\beta)I-\rho B^\intercal  B$ is
\begin{align*}
x^{k+1}&=\prox_{(1/\rho)f_1}\left((1/\rho)u^k+By^{k} \right)\\
y^{k+1}&=\prox_{\beta g_1}\left(y^k-\beta \nabla g_2(y^k)-\beta B^\intercal ( u ^k-\rho (x^{k+1}-By^{k}))\right)\\
 u ^{k+1}&= u ^{k}-\rho(x^{k+1}-By^{k+1}).
\end{align*}
Apply the Moreau identity to \textbf{recover Condat--V\~u}
\begin{align*}
\mu^{k+1}&=\prox_{\rho  f_1^*}\left(\mu^k+\rho B(2y^k-y^{k-1})\right)\\
y^{k+1}&=\prox_{\beta g_1}\left(y^k-\beta \nabla g_2(y^k)-\beta B^\intercal \mu^{k+1}\right).
\end{align*}
However, FLiP-ADMM condition $1\ge  \beta\rho\lambda_\mathrm{max}(B^\intercal  B)+3\beta L_g$ is worse than what we have in Ch3.
\end{frame}

\begin{frame}
Consider
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f_1(x)+f_2(x)+g_1(y)+g_2(y)\\
\mbox{subject to}&Ax+By=c.
\end{array}
\]
FLiP-ADMM with $P=(1/\alpha)I-\rho A^\intercal  A$ and $Q=(1/\beta)I-\rho B^\intercal  B$ is
\begin{align*}
x^{k+1}&=
\prox_{\alpha f_1}\left(x^k-\alpha\left(\nabla f_2(x^k)+ A^\intercal  u ^k+\rho A^\intercal (Ax^k+By^k-c)\right)\right)\\
y^{k+1}&=
\prox_{\beta g_1}
\left(y^k-\beta \left(\nabla g_2(y^k)+ B^\intercal  u ^k+\rho B^\intercal (Ax^{k+1}+By^k-c)
\right)
\right)\\
 u ^{k+1}&= u ^{k}+\varphi\rho(Ax^{k+1}+By^{k+1}-c).
\end{align*}
We call it \textbf{doubly-linearized ADMM}, which generalizes PDHG and Condat--V\~u.
\medskip

Converge if $1\ge \alpha \rho\lambda_\mathrm{max}(A^\intercal  A)+\alpha L_f$, $1\ge \beta \rho\lambda_\mathrm{max}(B^\intercal  B)+3\beta L_g$, and $0<\varphi<(1+\sqrt{5})/2$.
\end{frame}

\begin{frame}{Partial linearization}
Consider 
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f_2(x)+g_1(y)+g_2(y)\\
\mbox{subject to}&Ax+By=c.
\end{array}
\]

Assume 
\begin{itemize}
  \item $\gamma I+\rho A^\intercal  A$ is \emph{not} easily invertible
  \item $\gamma I+C$ is easily invertible for some $C\approx \rho A^\intercal  A$
\end{itemize}
\medskip

Choose $P=\gamma I+C -\rho A^\intercal  A$ where $\gamma>\lambda_\mathrm{max}(\rho A^\intercal  A-C)$ is small.
\medskip

Then, the $x$-update of FLiP-ADMM
\begin{align*}
x^{k+1}&=x^k-(\gamma I+C)^{-1} (\nabla f_2(x^k)+A^\intercal  u^k+\rho A^\intercal (Ax^k+By^k-c)),
\end{align*}
is easy to compute. Call it \textbf{partial linearization}. It reduces iterations compared to (full) linearization (with $P=\gamma I-\rho A^\intercal  A$).
\end{frame}


\begin{frame}{CT imaging with total variation regularization}
Let $x$ represent a 2D or 3D image to recover from CT measurements $b$:
\[
    \begin{array}{ll}
    \underset{x\in \reals^p}{\mbox{minimize}}&\displaystyle{\ell(Ax-b)+\lambda \|Dx\|_1},
    \end{array}
\]
where $A$ is the discrete Radon transform operator, $D$ is a finite difference operator, and $\ell$ is a CCP function.
\medskip

PDHG has low-cost steps and but requires too many iterations. Classic ADMM requires (much) fewer iterations but an expensive step:
\begin{align*}
  x^{k+1}&=x^k-(\rho A^\intercal  A+\rho D^\intercal  D)^{-1}\left(A^\intercal  u^{k+1}+ D v^{k+1}\right).
\end{align*}


Since $A^\intercal  A$ and $D^\intercal  D$ are discretizations of shift-invariant continuous operators, they can be approximated by circulant matrices, so we use a \emph{circulant matrix}
\[
C \approx \rho A^\intercal  A + \rho D^\intercal  D.
\]
\end{frame}


\begin{frame}

Let $c^\intercal$ be the first row of $C$ and $\hat{c}$ is its discrete Fourier transform. Let $F(\cdot)$ be a discrete Fourier transform. By the \emph{convolution theorem}, for any vector $x$ and its inverse Fourier transform $\breve{x}$, we have
\begin{align*}
(\gamma I + C)x & = F(\mathrm{Diag}(\gamma \vone + \hat{c})\breve{x})\\
(\gamma I + C)^{-1}x & = F(\mathrm{Diag}^{-1}(\gamma \vone + \hat{c})\breve{x}),
\end{align*}
so $(\gamma I + C)^{-1}$ is easy by fast Fourier transform (FFT).
\medskip

FLiP-ADMM with partial linearization $P=\gamma I + C - \rho A^\intercal  A - \rho D^\intercal  D$
\begin{align*}
u^{k+1}&=\prox_{\rho \ell^*}\left(u^k+\rho A(2x^k-x^{k-1})-\rho b\right)\\
v^{k+1}&=\proj_{[-\lambda,\lambda]}\left(v^{k+1}+\rho D(2x^k-x^{k-1})\right)\\
x^{k+1}&=x^k-(\gamma I+C)^{-1}\left(A^\intercal  u^{k+1}+ D v^{k+1}\right)
\end{align*}
has easy-to-compute steps. A small $\gamma > \lambda_{\max}(\rho A^\intercal  A + \rho D^\intercal  D - C)$ leads a minimal increase in iterations over classic ADMM.
\end{frame}

\begin{frame}{Multi-block ADMM problem}
Partition $x\in \reals^p$ into $m$ non-overlapping blocks of sizes $p_1,\dots,p_m$. 
\medskip

Partition matrix $A=
\begin{bmatrix}
A_{:,1}&A_{:,2}&\cdots &A_{:,m}
\end{bmatrix}
$
such that 
\[
Ax=A_{:,1}x_1+A_{:,2}x_2+\dots +A_{:,m}x_m.
\]

Multi-block ADMM problem or extended monotropic program is
\begin{equation}
\begin{array}{ll}
\underset{\substack{(x_1,\dots,x_m)\in \reals^p}}{\mbox{minimize}}&
\displaystyle{f_1(x_1) + f_2(x_2)+ \dots + f_m(x_m)}
\\
\mbox{subject to}&A_{:,1}x_1+A_{:,2}x_2+\dots +A_{:,m}x_m=c.
\end{array}
\label{eq:block_split_problem}
\end{equation}

Unless the column-blocks of $A$ are orthogonal, i.e., $A^\intercal _{:,i}A_{:,j}=0$ for all $i\ne j$, the blocks $x_1^{k+1},\dots,x_m^{k+1}$ cannot be computed independently.
\medskip

Next, we present two splitting techniques with which $x_1^{k+1},\dots,x_m^{k+1}$ can be computed independently in parallel.
\end{frame}

\begin{frame}{Jacobi ADMM}
In numerical linear algebra, the Jacobi method is an iterative method for solving certain linear systems. It updates the blocks of $x$ independently.
\bigskip

Consider problem \eqref{eq:block_split_problem} and matrix
\[
P=
\begin{bmatrix}
\gamma I &-\rho A_{:,1}^\intercal  A_{:,2} &\cdots&\cdots&-\rho A_{:,1}^\intercal  A_{:,m}
\\
-\rho A_{:,2}^\intercal  A_{:,1}&\gamma I&\cdots&\cdots&-\rho A_{:,2}^\intercal  A_{:,m}
\\
\vdots &&\ddots&&\vdots\\
\vdots &&&\ddots&\vdots\\
-\rho A_{:,m}^\intercal  A_{:,1}&-\rho A_{:,m}^\intercal  A_{:,2}&\cdots& -\rho A_{:,m}^\intercal  A_{:,(m-1)}&\gamma I
\end{bmatrix},
\]
which is positive semidefinite for $\gamma\ge \rho \lambda_\mathrm{max}(A^\intercal  A)$.
\end{frame}

\begin{frame}
Let
\[
\lagrange_\rho(x,u)=
\sum^m_{i=1}f_i(x_i)
+
\left<u,Ax-c\right>+\frac{\rho}{2}\|Ax-c\|^2.
\]
Let $x_{\neq i}^k$ denote all components of $x^k$ excluding $x_i^k$.
FLiP-ADMM with the matrix $P$ is
\begin{align*}
x^{k+1}_i &= \argmin_{x_i\in \reals^{p_i}}
\left\{\lagrange_\rho (x_i,x_{\neq i}^k,u^k)+\frac{\gamma}{2}\|x_i-x_i^k\|^2\right\}
\qquad \text{ for }i=1,\dots,m
\\
u^{k+1}&=u^k+\varphi\rho \left(Ax^{k+1}-c\right).
\end{align*}
This method is called \textbf{Jacobi ADMM} in analogy to the Jacobi method.
\bigskip

See Exercise 8.3 for other choices of $P$, where the diagonal $\gamma I$ is replaced by diagonal blocks.
\end{frame}

\begin{frame}{Dummy variable technique + FLiP-ADMM}
Consider the following generalization to problem \eqref{eq:block_split_problem}:
\[
\begin{array}{ll}
\underset{\substack{(x_1,\dots,x_m)\in \reals^p\\y\in \reals^n}}{\mbox{minimize}}&\displaystyle{\sum^m_{i=1}f_i(x_i)}+g(y)
\\
\mbox{subject to}&Ax+y=c.
\end{array}
\]
Introduce dummy variables $z_1,\dots,z_m$ and eliminate $y$ to get the equivalent problem
\[
\begin{array}{ll}
\underset{\substack{(x_1,\dots,x_m)\in \reals^p\\z_1,\dots,z_m\in \reals^n}}{\mbox{minimize}}&\displaystyle{\sum^m_{i=1}f_i(x_i)}+g\left(c-\sum^m_{i=1}z_i\right)\\
\mbox{subject to}&A_{:,i}x_i-z_i=0\qquad\text{ for }i=1,\dots,m.
\end{array}
\]
\end{frame}

\begin{frame}
Apply FLiP-ADMM with $P=0$, $Q=0$, no function linearization, and initial $u$-variables satisfying $u_1^0=\dots=u_m^0$. Then we can show $u_1^k=\dots=u_m^k$ for $k=1,\dots,m$, and the iteration simplifies to
\begin{alignat*}{3}
x^{k+1}_i &\in \argmin_{x_i\in \reals^{p_i}} \left\{\hspace{-2pt} f_i(x_i)
\hspace{-2pt}+\hspace{-2pt}
\left<\hspace{-2pt} u^k
\hspace{-2pt}+\hspace{-2pt}
\frac{\rho}{m}(Ax^{k}-z_\mathrm{sum}^k), A_{:,i}x_i\hspace{-2pt}\right> \hspace{-2pt}+\hspace{-2pt}
\frac{\rho}{2}\left\|A_{:,i}(x_i-x_i^k)\right\|^2 \hspace{-2pt}\right\}\\[-0.6em]
&\hspace{2.7in}\text{for }i=1,\dots,m\\[-1.em]
%z_\mathrm{sum}^{k+1} &\in \argmin_{z_\mathrm{sum}\in \reals^{n}} \left\{mg(c-z_\mathrm{sum})- \langle u^k_\mathrm{sum},z_\mathrm{sum}\rangle +\frac{\rho}{2}\|Ax^{k+1}-z_\mathrm{sum}\|^2\right\}\\
z_\mathrm{sum}^{k+1}&=c-\prox_{\frac{m}{\rho}g}\left(c-Ax^{k+1}-\frac{m}{\rho}u^k\right)\\
u^{k+1}&=u^{k}+\frac{\varphi\rho}{m}\left(Ax^{k+1}-z_\mathrm{sum}^{k+1}\right).
\end{alignat*}
The method converges if $\varphi\in(0,(1+\sqrt{5})/2)$.
\end{frame}

\begin{frame}{Consensus technique + FLiP-ADMM}
Consider
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p}}{\mbox{minimize}}&\displaystyle{\sum^n_{i=1}f_i(x)}.
\end{array}
\]
Use the consensus technique to get the equivalent problem
\[
\begin{array}{ll}
\underset{\substack{x_1,\dots,x_n,z\in \reals^p}}{\mbox{minimize}}&\displaystyle{\sum^n_{i=1}f_i(x_i)}\\
\mbox{subject to}&x_i=z,\qquad  \text{for }i=1,\dots,n.
\end{array}
\]

Here, $x_i\in\reals^p$ is a copy of $x\in\reals^p$. This contrasts with block splitting, where each $x_i$ represented a single block of $x$.
\end{frame}

\begin{frame}

Apply FLiP-ADMM with $P=0$, $Q=0$, no function linearization, and initial $u$-variables satisfying  $u^0_1+\dots+u^0_n=0$ to get
\begin{alignat*}{3}
x^{k+1}_i&=\argmin_{x\in\reals^p}\left\{f_i(x_i)+\langle u^k_i,x_i \rangle+\frac{\rho}{2}\|x_i-z^k\|^2\right\}
&&\qquad\text{for }i=1,\dots,n
\\
%z^{k+1}&=\argmin_{z\in\reals^p}\left\{-\sum^n_{i=1}\langle u^k_i,z \rangle+\frac{\rho}{2}\sum^n_{i=1}\|z-x_i^{k+1}\|^2\right\}\\
%z^{k+1}&=\frac{1}{n}\sum^n_{i=1}\left(x_i+\frac{1}{\rho}u_i\right)\\
z^{k+1}&=\frac{1}{n}\sum^n_{i=1}x_i^{k+1}\\
u^{k+1}_i&=u^k_i+\varphi \rho(x_i^{k+1}-z^{k+1})
&&\qquad\text{for }i=1,\dots,n.
\end{alignat*}
Converge if  $\varphi\in(0,(1+\sqrt{5})/2)$.
\bigskip

The consensus technique is versatile. Instead of constraining $x_1,\dots,x_n$ to equal a single $z$ here, we will equal them to multiple $z$-variables through a graph structure to obtain a decentralized method in CH11.
\end{frame}

\begin{frame}{2-1-2 ADMM}
This is a technique that applies an ADMM method to a problem with one or two more blocks (if they are strongly-convex quadratic) than what it is designed for.
\bigskip

Assume $g$ is a strongly convex quadratic function with affine constraints, i.e.,
\[
g(y)=y^\intercal  My+\mu^\intercal y+\delta_{\{y\in \reals^q\,|\,Ny=\nu\}}(y)
\]
for $M\in \reals^{q\times q}$ with $M\succ 0$, $N\in\reals^{s\times q}$, and $\nu\in\cR(N)$. If no affine constraint, we set $s=0$.
\bigskip


Define
\begin{align*}
  \lagrange_\rho (x,y,u) & = f(x) + g(y) +\langle u,Ax+By-c\rangle  + \frac{\rho}{2}\|Ax+By-c\|^2.
\end{align*}
\end{frame}

\begin{frame}
\textbf{2-1-2 ADMM} is the method:
\begin{align*}
y^{k+1/2} &= \argmin_{y\in \reals^q}\lagrange_\rho (x^k,y,u^k) \\
x^{k+1} &\in \argmin_{x\in \reals^p} \lagrange_\rho (x,y^{k+\frac{1}{2}},u^k)\\
y^{k+1} &= \argmin_{y\in \reals^q}\lagrange_\rho (x^{k+1},y,u^k)  \\
u^{k+1} &= u^k + \varphi\rho(Ax^{k+1}+By^{k+1}-c).
\end{align*}

It is equivalent to (single-block) FLiP-ADMM applied to 
\[  \begin{alignedat}{2}
     (x^{k+1},y^{k+1}) & \in\argmin_{x\in\reals^p,\,y\in\reals^q}\left\{ \lagrange_\rho (x,y,u^k)+ \frac{\rho}{2}\|x-x^{k}\|_{P}^2 \right\}\\
     u^{k+1} & = u^k + \varphi\rho(Ax^{k+1}+By^{k+1}-c)
  \end{alignedat}
\]
with $P=A^\intercal  BTB^\intercal  A$. Converge if $\varphi\in(0,2)$.
\medskip

See Exercises 8.11 for 4-block ADMM with 2-1-2-4-3-4 updates and 8.12 for generalization with function linearization and proximal terms.
\end{frame}

\begin{frame}{Trip-ADMM}
Consider the more problem
\[
\begin{array}{ll}
\underset{\substack{x\in \reals^p,\,y\in \reals^q}}{\mbox{minimize}}&f_1(Cx)+f_2(x)+g_1(Dy)+g_2(y)\\
\mbox{subject to}&Ax+By=c.
\end{array}
\]
\textbf{Trip-ADMM} (Triple-linearized ADMM)  is the method
\begin{align*}
&\left\{
  \begin{alignedat}{2}
    x^{k+1/2}&=x^k- \sigma\left(C^\intercal v^{k}+\nabla f_2(x^k) + A^\intercal  u^k+ \rho A^\intercal ( Ax^k+By^k-c)\right)\\
    v^{k+1}&=\prox_{\tau f_1^*}\left(v^k+\tau Cx^{k+1/2}\right)\\
    x^{k+1}&=x^{k+1/2}-\sigma C^\intercal \left(v^{k+1}-v^k\right)
  \end{alignedat}
\right. \\
% y^{k+1/2}&=y^k-\sigma\left(D^\intercal w^{k}+\nabla g_2(y^k) + B^\intercal  u^k+\rho B^\intercal (Ax^{k+1}+By^k-c)\right)\\
% w^{k+1}&=\prox_{\tau g_1^*}\left(w^k+\tau Dy^{k+1/2}\right)\\
% y^{k+1}&=y^{k+1/2}-\sigma D^\intercal \left(w^{k+1}-w^k\right)\\[0.7em]
&\left\{
  \begin{alignedat}{2}
    y^{k+1/2}&=y^k-\sigma\left(D^\intercal w^{k}+\nabla g_2(y^k) + B^\intercal  u^k+\rho B^\intercal (Ax^{k+1}+By^k-c)\right)\\
    w^{k+1}&=\prox_{\tau g_1^*}\left(w^k+\tau Dy^{k+1/2}\right)\\
    y^{k+1}&=y^{k+1/2}-\sigma D^\intercal \left(w^{k+1}-w^k\right)
  \end{alignedat}
\right. \\
&\hspace{18pt}u^{k+1}=u^k+\rho\left(Ax^{k+1}+By^{k+1}-c\right),
\end{align*}
which generalizes FLiP-ADMM. 
\end{frame}

\begin{frame}
If the parameters satisfy $\rho>0$, $\sigma>0$, $\tau>0$,
\begin{gather*}
1\ge \sigma\rho \lambda_\mathrm{max}(A^\intercal  A)+\sigma L_f,\qquad
1\ge \sigma\rho \lambda_\mathrm{max}(B^\intercal  B)+3\sigma L_g,\\
1\ge \sigma\tau \lambda_\mathrm{max}(CC^\intercal ),\qquad
1\ge \sigma\tau \lambda_\mathrm{max}(DD^\intercal )
\end{gather*}
and assume total duality, we have
\begin{align*}
  &f_1\left(Cx^{k}-\sigma \tilde{C}^\intercal \tilde{C}(v^{k+1}-v^{k})\right)+f_2(x^{k}) \\
  & +g_1\left(Dy^{k}-\sigma \tilde{D}^\intercal \tilde{D}(w^{k+1}-w^{k})\right)+g_2(y^{k})\\
 &\rightarrow f_1(Cx^\star)+f_2(x^\star)+g_1(Dy^\star)+g_2(y^\star),\\[5pt]
%\tilde{C}^\intercal \tilde{C}(v^{k+1}-v^{k})\rightarrow 0,\qquad \tilde{D}^\intercal \tilde{D}(w^{k+1}-w^{k})\rightarrow 0\\
& Ax^k+Bx^k-c\rightarrow 0.
\end{align*}
\end{frame}

%\begin{frame}{Relationship to Split-Bregman method}
%\end{frame}

\begin{frame}{Conclusion}
FLiP-ADMM is a combination of four techniques: alternating update, method of multipliers, linearization, function-linearization, and use of proximal terms.
\bigskip

These techniques can be combined like modules to solver problems with complicated structures.
\bigskip

ADMM-type methods are ``splitting methods'', intimately related to monotone operator methods, though are not monotone operator methods themselves.
\bigskip

Fully general FLiP-ADMM (with dual extrapolation $\varphi$) cannot be reduced to a monotone operator splitting method and must be analyzed directly.
\end{frame}

\end{document}
